/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
Traceback (most recent call last):
  File "get_flops.py", line 60, in <module>
    main()
  File "get_flops.py", line 50, in main
    flops, params = get_model_complexity_info(model, input_shape)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/cnn/utils/flops_counter.py", line 106, in get_model_complexity_info
    _ = flops_model(batch)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1148, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 118, in forward_dummy
    seg_logit = self.encode_decode(img, None)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 73, in encode_decode
    x = self.extract_feat(img)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 65, in extract_feat
    x = self.backbone(img)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 416, in forward
    x = layer(x)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 121, in forward
    x = _inner_forward(x)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 114, in _inner_forward
    x = self.attn(self.norm1(x), identity=x)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py", line 546, in forward
    key_padding_mask=key_padding_mask)[0]
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 1160, in forward
    attn_mask=attn_mask, average_attn_weights=average_attn_weights)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 5179, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 4856, in _scaled_dot_product_attention
    attn = softmax(attn, dim=-1)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 1834, in softmax
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 31.75 GiB total capacity; 27.97 GiB already allocated; 1.15 GiB free; 29.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "get_flops.py", line 60, in <module>
    main()
  File "get_flops.py", line 35, in main
    cfg = Config.fromfile(args.config)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/utils/config.py", line 341, in fromfile
    use_predefined_variables)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/utils/config.py", line 183, in _file2dict
    check_file_exist(filename)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/utils/path.py", line 23, in check_file_exist
    raise FileNotFoundError(msg_tmpl.format(filename))
FileNotFoundError: file "/work/kcarvajal/mmsegmentation_Nematodos/tools/configs/_nematodos_/segmenter/segmenterL_A2.py" does not exist
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
Traceback (most recent call last):
  File "get_flops.py", line 60, in <module>
    main()
  File "get_flops.py", line 50, in main
    flops, params = get_model_complexity_info(model, input_shape)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/cnn/utils/flops_counter.py", line 106, in get_model_complexity_info
    _ = flops_model(batch)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1148, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 118, in forward_dummy
    seg_logit = self.encode_decode(img, None)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 73, in encode_decode
    x = self.extract_feat(img)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 65, in extract_feat
    x = self.backbone(img)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 416, in forward
    x = layer(x)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 121, in forward
    x = _inner_forward(x)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 114, in _inner_forward
    x = self.attn(self.norm1(x), identity=x)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py", line 546, in forward
    key_padding_mask=key_padding_mask)[0]
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 1160, in forward
    attn_mask=attn_mask, average_attn_weights=average_attn_weights)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 5179, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 4856, in _scaled_dot_product_attention
    attn = softmax(attn, dim=-1)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 1834, in softmax
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 31.75 GiB total capacity; 27.25 GiB already allocated; 2.97 GiB free; 27.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
Traceback (most recent call last):
  File "get_flops.py", line 60, in <module>
    main()
  File "get_flops.py", line 50, in main
    flops, params = get_model_complexity_info(model, input_shape)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/cnn/utils/flops_counter.py", line 106, in get_model_complexity_info
    _ = flops_model(batch)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1148, in _call_impl
    result = forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 118, in forward_dummy
    seg_logit = self.encode_decode(img, None)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 73, in encode_decode
    x = self.extract_feat(img)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/segmentors/encoder_decoder.py", line 65, in extract_feat
    x = self.backbone(img)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 416, in forward
    x = layer(x)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 121, in forward
    x = _inner_forward(x)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 114, in _inner_forward
    x = self.attn(self.norm1(x), identity=x)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/utils/misc.py", line 340, in new_func
    output = old_func(*args, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py", line 546, in forward
    key_padding_mask=key_padding_mask)[0]
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 1160, in forward
    attn_mask=attn_mask, average_attn_weights=average_attn_weights)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 5179, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 4856, in _scaled_dot_product_attention
    attn = softmax(attn, dim=-1)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/functional.py", line 1834, in softmax
    ret = input.softmax(dim)
RuntimeError: CUDA out of memory. Tried to allocate 3.00 GiB (GPU 0; 31.75 GiB total capacity; 27.25 GiB already allocated; 2.97 GiB free; 27.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-09-21 17:22:27,024 - mmseg - INFO - Multi-processing start method is `None`
2022-09-21 17:22:27,028 - mmseg - INFO - OpenCV num_threads is `48
2022-09-21 17:22:27,410 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.27.0+45dfd3f
------------------------------------------------------------

2022-09-21 17:22:27,411 - mmseg - INFO - Distributed training: False
2022-09-21 17:22:28,126 - mmseg - INFO - Config:
checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth'
backbone_norm_cfg = dict(type='LN', eps=1e-06, requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=
    'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=(640, 640),
        patch_size=16,
        in_channels=3,
        embed_dims=1024,
        num_layers=24,
        num_heads=16,
        drop_path_rate=0.1,
        attn_drop_rate=0.0,
        drop_rate=0.0,
        final_norm=True,
        norm_cfg=dict(type='LN', eps=1e-06, requires_grad=True),
        with_cls_token=True,
        interpolate_mode='bicubic'),
    decode_head=dict(
        type='SegmenterMaskTransformerHead',
        in_channels=1024,
        channels=1024,
        num_classes=2,
        num_layers=2,
        num_heads=16,
        embed_dims=1024,
        dropout_ratio=0.0,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    test_cfg=dict(mode='slide', crop_size=(640, 640), stride=(608, 608)))
dataset_type = 'NematodosDataset'
data_root = '../data/nematodos'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (768, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
    dict(type='RandomRotate', prob=0.75, degree=30),
    dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 768),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=2,
    train=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/test.txt'),
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=2, shuffle=False))
log_config = dict(
    interval=16000,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False, interval=8000),
        dict(
            type='MMSegWandbHook',
            with_step=False,
            init_kwargs=dict(
                entity='seg_nematodos',
                project='Nematodos',
                name='segmenterL_base',
                id='segmenterL_base',
                resume='allow',
                notes=
                'Entrenamiento modelo segmenterL, batch=1, lr=0.001, m=0.9, 160k iter'
            ),
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1), ('val', 1)]
cudnn_benchmark = True
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0)
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000, max_keep_ckpts=2)
evaluation = dict(
    interval=16000, metric=['mIoU', 'mDice', 'mFscore'], pre_eval=True)
work_dir = '../work_dirs/segmenterL'
seed = 0
gpu_ids = [0]
device = 'cuda'
auto_resume = False

2022-09-21 17:22:28,127 - mmseg - INFO - Set random seed to 715898160, deterministic: False
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
2022-09-21 17:22:30,295 - mmseg - INFO - load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth
Downloading: "https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth" to /home/kcarvajal/.cache/torch/hub/checkpoints/vit_large_p16_384_20220308-d4efb41d.pth
  0%|          | 0.00/1.13G [00:00<?, ?B/s]  0%|          | 56.0k/1.13G [00:00<38:10, 530kB/s]  0%|          | 112k/1.13G [00:00<37:23, 541kB/s]   0%|          | 232k/1.13G [00:00<24:06, 839kB/s]  0%|          | 464k/1.13G [00:00<14:11, 1.43MB/s]  0%|          | 912k/1.13G [00:00<07:57, 2.54MB/s]  0%|          | 1.76M/1.13G [00:00<04:16, 4.73MB/s]  0%|          | 3.45M/1.13G [00:00<02:15, 8.94MB/s]  0%|          | 4.54M/1.13G [00:00<02:05, 9.68MB/s]  0%|          | 5.47M/1.13G [00:00<02:07, 9.45MB/s]  1%|          | 6.38M/1.13G [00:01<02:15, 8.93MB/s]  1%|          | 7.23M/1.13G [00:01<02:19, 8.68MB/s]  1%|          | 8.10M/1.13G [00:01<02:17, 8.75MB/s]  1%|          | 8.95M/1.13G [00:01<02:25, 8.29MB/s]  1%|          | 9.90M/1.13G [00:01<02:17, 8.75MB/s]  1%|          | 10.7M/1.13G [00:01<02:27, 8.16MB/s]  1%|          | 11.7M/1.13G [00:01<02:18, 8.67MB/s]  1%|          | 12.6M/1.13G [00:01<02:28, 8.08MB/s]  1%|          | 13.5M/1.13G [00:01<02:18, 8.66MB/s]  1%|          | 14.4M/1.13G [00:02<02:28, 8.09MB/s]  1%|▏         | 15.2M/1.13G [00:02<02:28, 8.09MB/s]  1%|▏         | 16.0M/1.13G [00:02<02:24, 8.31MB/s]  1%|▏         | 16.8M/1.13G [00:02<02:28, 8.06MB/s]  2%|▏         | 17.7M/1.13G [00:02<02:24, 8.30MB/s]  2%|▏         | 18.5M/1.13G [00:02<02:27, 8.13MB/s]  2%|▏         | 19.3M/1.13G [00:02<02:24, 8.24MB/s]  2%|▏         | 20.1M/1.13G [00:02<02:24, 8.27MB/s]  2%|▏         | 20.9M/1.13G [00:02<02:25, 8.20MB/s]  2%|▏         | 21.7M/1.13G [00:02<02:22, 8.39MB/s]  2%|▏         | 22.5M/1.13G [00:03<02:25, 8.19MB/s]  2%|▏         | 23.4M/1.13G [00:03<02:23, 8.28MB/s]  2%|▏         | 24.2M/1.13G [00:03<02:23, 8.30MB/s]  2%|▏         | 25.0M/1.13G [00:03<02:24, 8.21MB/s]  2%|▏         | 25.8M/1.13G [00:03<02:21, 8.41MB/s]  2%|▏         | 26.6M/1.13G [00:03<02:25, 8.17MB/s]  2%|▏         | 27.4M/1.13G [00:03<02:22, 8.34MB/s]  2%|▏         | 28.2M/1.13G [00:03<02:24, 8.21MB/s]  3%|▎         | 29.1M/1.13G [00:03<02:23, 8.26MB/s]  3%|▎         | 29.9M/1.13G [00:04<02:20, 8.40MB/s]  3%|▎         | 30.7M/1.13G [00:04<04:05, 4.82MB/s]  3%|▎         | 32.6M/1.13G [00:04<03:14, 6.07MB/s]  3%|▎         | 33.5M/1.13G [00:04<02:59, 6.56MB/s]  3%|▎         | 34.3M/1.13G [00:04<02:48, 7.01MB/s]  3%|▎         | 35.1M/1.13G [00:04<02:45, 7.14MB/s]  3%|▎         | 35.9M/1.13G [00:05<02:36, 7.53MB/s]  3%|▎         | 36.7M/1.13G [00:05<02:35, 7.56MB/s]  3%|▎         | 37.5M/1.13G [00:05<02:29, 7.87MB/s]  3%|▎         | 38.4M/1.13G [00:05<02:24, 8.11MB/s]  3%|▎         | 39.2M/1.13G [00:05<02:27, 7.96MB/s]  3%|▎         | 40.0M/1.13G [00:05<02:23, 8.20MB/s]  4%|▎         | 40.8M/1.13G [00:05<02:25, 8.03MB/s]  4%|▎         | 41.7M/1.13G [00:05<02:22, 8.22MB/s]  4%|▎         | 42.5M/1.13G [00:05<02:24, 8.10MB/s]  4%|▎         | 43.3M/1.13G [00:05<02:22, 8.24MB/s]  4%|▍         | 44.1M/1.13G [00:06<02:19, 8.40MB/s]  4%|▍         | 44.9M/1.13G [00:06<02:23, 8.16MB/s]  4%|▍         | 45.8M/1.13G [00:06<02:19, 8.35MB/s]  4%|▍         | 46.6M/1.13G [00:06<02:23, 8.15MB/s]  4%|▍         | 47.4M/1.13G [00:06<02:20, 8.30MB/s]  4%|▍         | 48.2M/1.13G [00:06<02:17, 8.46MB/s]  4%|▍         | 49.1M/1.13G [00:06<02:21, 8.22MB/s]  4%|▍         | 49.9M/1.13G [00:06<02:19, 8.35MB/s]  4%|▍         | 50.7M/1.13G [00:06<02:22, 8.16MB/s]  4%|▍         | 51.5M/1.13G [00:07<02:19, 8.32MB/s]  5%|▍         | 52.3M/1.13G [00:07<02:22, 8.15MB/s]  5%|▍         | 53.1M/1.13G [00:07<02:19, 8.30MB/s]  5%|▍         | 54.0M/1.13G [00:07<02:17, 8.43MB/s]  5%|▍         | 54.8M/1.13G [00:07<02:21, 8.20MB/s]  5%|▍         | 55.6M/1.13G [00:07<02:18, 8.33MB/s]  5%|▍         | 56.4M/1.13G [00:07<02:21, 8.16MB/s]  5%|▍         | 57.3M/1.13G [00:07<02:18, 8.32MB/s]  5%|▌         | 58.1M/1.13G [00:07<02:20, 8.22MB/s]  5%|▌         | 58.9M/1.13G [00:07<02:21, 8.13MB/s]  5%|▌         | 59.7M/1.13G [00:08<02:16, 8.43MB/s]  5%|▌         | 60.5M/1.13G [00:08<02:19, 8.23MB/s]  5%|▌         | 61.4M/1.13G [00:08<02:17, 8.38MB/s]  5%|▌         | 62.2M/1.13G [00:08<02:20, 8.16MB/s]  5%|▌         | 63.0M/1.13G [00:08<02:19, 8.23MB/s]  6%|▌         | 63.9M/1.13G [00:08<02:15, 8.46MB/s]  6%|▌         | 64.7M/1.13G [00:08<02:18, 8.26MB/s]  6%|▌         | 65.5M/1.13G [00:08<02:16, 8.37MB/s]  6%|▌         | 66.3M/1.13G [00:08<02:20, 8.16MB/s]  6%|▌         | 67.1M/1.13G [00:08<02:17, 8.33MB/s]  6%|▌         | 67.9M/1.13G [00:09<02:19, 8.19MB/s]  6%|▌         | 68.8M/1.13G [00:09<02:18, 8.28MB/s]  6%|▌         | 69.6M/1.13G [00:09<02:15, 8.45MB/s]  6%|▌         | 70.4M/1.13G [00:09<02:19, 8.21MB/s]  6%|▌         | 71.3M/1.13G [00:09<02:16, 8.35MB/s]  6%|▌         | 72.1M/1.13G [00:09<02:19, 8.14MB/s]  6%|▋         | 72.9M/1.13G [00:09<02:17, 8.29MB/s]  6%|▋         | 73.7M/1.13G [00:09<02:17, 8.30MB/s]  6%|▋         | 74.5M/1.13G [00:09<02:17, 8.29MB/s]  7%|▋         | 75.3M/1.13G [00:10<02:15, 8.40MB/s]  7%|▋         | 76.1M/1.13G [00:10<02:19, 8.15MB/s]  7%|▋         | 77.0M/1.13G [00:10<02:16, 8.32MB/s]  7%|▋         | 77.8M/1.13G [00:10<02:17, 8.22MB/s]  7%|▋         | 78.5M/1.13G [00:10<02:17, 8.23MB/s]  7%|▋         | 79.4M/1.13G [00:10<02:14, 8.44MB/s]  7%|▋         | 80.2M/1.13G [00:10<02:17, 8.23MB/s]  7%|▋         | 81.1M/1.13G [00:10<02:15, 8.35MB/s]  7%|▋         | 81.9M/1.13G [00:10<02:18, 8.14MB/s]  7%|▋         | 82.7M/1.13G [00:10<02:16, 8.25MB/s]  7%|▋         | 83.5M/1.13G [00:11<02:16, 8.26MB/s]  7%|▋         | 84.3M/1.13G [00:11<02:15, 8.30MB/s]  7%|▋         | 85.1M/1.13G [00:11<02:13, 8.41MB/s]  7%|▋         | 85.9M/1.13G [00:11<02:17, 8.17MB/s]  7%|▋         | 86.8M/1.13G [00:11<02:14, 8.36MB/s]  8%|▊         | 87.6M/1.13G [00:11<02:16, 8.21MB/s]  8%|▊         | 88.4M/1.13G [00:11<02:16, 8.21MB/s]  8%|▊         | 89.2M/1.13G [00:11<02:12, 8.47MB/s]  8%|▊         | 90.1M/1.13G [00:11<02:16, 8.21MB/s]  8%|▊         | 90.9M/1.13G [00:11<02:13, 8.36MB/s]  8%|▊         | 91.7M/1.13G [00:12<02:17, 8.16MB/s]  8%|▊         | 92.5M/1.13G [00:12<02:14, 8.29MB/s]  8%|▊         | 93.3M/1.13G [00:12<02:15, 8.24MB/s]  8%|▊         | 94.1M/1.13G [00:12<02:14, 8.31MB/s]  8%|▊         | 95.0M/1.13G [00:12<02:12, 8.40MB/s]  8%|▊         | 95.8M/1.13G [00:12<02:16, 8.16MB/s]  8%|▊         | 96.6M/1.13G [00:12<02:13, 8.35MB/s]  8%|▊         | 97.4M/1.13G [00:12<02:16, 8.17MB/s]  8%|▊         | 98.2M/1.13G [00:12<02:15, 8.20MB/s]  9%|▊         | 99.1M/1.13G [00:13<02:11, 8.45MB/s]  9%|▊         | 99.9M/1.13G [00:13<02:14, 8.25MB/s]  9%|▊         | 101M/1.13G [00:13<02:12, 8.38MB/s]   9%|▉         | 102M/1.13G [00:13<02:16, 8.13MB/s]  9%|▉         | 102M/1.13G [00:13<02:13, 8.31MB/s]  9%|▉         | 103M/1.13G [00:13<02:14, 8.21MB/s]  9%|▉         | 104M/1.13G [00:13<02:13, 8.26MB/s]  9%|▉         | 105M/1.13G [00:14<04:45, 3.88MB/s]  9%|▉         | 106M/1.13G [00:14<04:24, 4.18MB/s]  9%|▉         | 107M/1.13G [00:14<03:43, 4.93MB/s]  9%|▉         | 108M/1.13G [00:14<03:15, 5.62MB/s]  9%|▉         | 108M/1.13G [00:14<03:02, 6.03MB/s]  9%|▉         | 109M/1.13G [00:14<02:45, 6.67MB/s]  9%|▉         | 110M/1.13G [00:14<02:39, 6.89MB/s] 10%|▉         | 111M/1.13G [00:15<02:28, 7.39MB/s] 10%|▉         | 112M/1.13G [00:15<02:21, 7.76MB/s] 10%|▉         | 112M/1.13G [00:15<02:22, 7.72MB/s] 10%|▉         | 113M/1.13G [00:15<02:16, 8.03MB/s] 10%|▉         | 114M/1.13G [00:15<02:18, 7.89MB/s] 10%|▉         | 115M/1.13G [00:15<02:14, 8.16MB/s] 10%|▉         | 116M/1.13G [00:15<02:16, 8.04MB/s] 10%|█         | 116M/1.13G [00:15<02:12, 8.22MB/s] 10%|█         | 117M/1.13G [00:15<02:10, 8.38MB/s] 10%|█         | 118M/1.13G [00:15<02:13, 8.16MB/s] 10%|█         | 119M/1.13G [00:16<02:10, 8.35MB/s] 10%|█         | 120M/1.13G [00:16<02:14, 8.11MB/s] 10%|█         | 121M/1.13G [00:16<02:11, 8.30MB/s] 10%|█         | 121M/1.13G [00:16<02:09, 8.42MB/s] 11%|█         | 122M/1.13G [00:16<02:12, 8.20MB/s] 11%|█         | 123M/1.13G [00:16<02:09, 8.40MB/s] 11%|█         | 124M/1.13G [00:16<02:13, 8.15MB/s] 11%|█         | 125M/1.13G [00:16<02:10, 8.33MB/s] 11%|█         | 126M/1.13G [00:16<02:13, 8.11MB/s] 11%|█         | 126M/1.13G [00:16<02:10, 8.31MB/s] 11%|█         | 127M/1.13G [00:17<02:07, 8.46MB/s] 11%|█         | 128M/1.13G [00:17<02:11, 8.21MB/s] 11%|█         | 129M/1.13G [00:17<02:09, 8.33MB/s] 11%|█         | 130M/1.13G [00:17<02:12, 8.16MB/s] 11%|█▏        | 131M/1.13G [00:17<02:09, 8.33MB/s] 11%|█▏        | 131M/1.13G [00:17<02:12, 8.13MB/s] 11%|█▏        | 132M/1.13G [00:17<02:09, 8.31MB/s] 11%|█▏        | 133M/1.13G [00:17<02:07, 8.41MB/s] 12%|█▏        | 134M/1.13G [00:17<02:10, 8.21MB/s] 12%|█▏        | 135M/1.13G [00:18<02:08, 8.37MB/s] 12%|█▏        | 135M/1.13G [00:18<02:11, 8.14MB/s] 12%|█▏        | 136M/1.13G [00:18<02:09, 8.30MB/s] 12%|█▏        | 137M/1.13G [00:18<02:06, 8.45MB/s] 12%|█▏        | 138M/1.13G [00:18<02:09, 8.24MB/s] 12%|█▏        | 139M/1.13G [00:18<02:07, 8.39MB/s] 12%|█▏        | 140M/1.13G [00:18<02:11, 8.12MB/s] 12%|█▏        | 140M/1.13G [00:18<02:07, 8.35MB/s] 12%|█▏        | 141M/1.13G [00:18<02:11, 8.09MB/s] 12%|█▏        | 142M/1.13G [00:19<04:50, 3.67MB/s] 13%|█▎        | 146M/1.13G [00:19<01:46, 9.98MB/s] 13%|█▎        | 148M/1.13G [00:19<02:24, 7.34MB/s] 13%|█▎        | 149M/1.13G [00:20<02:32, 6.96MB/s] 13%|█▎        | 150M/1.13G [00:20<02:25, 7.28MB/s] 13%|█▎        | 151M/1.13G [00:20<02:22, 7.40MB/s] 13%|█▎        | 152M/1.13G [00:20<02:17, 7.67MB/s] 13%|█▎        | 153M/1.13G [00:20<02:16, 7.72MB/s] 13%|█▎        | 154M/1.13G [00:20<02:12, 7.92MB/s] 13%|█▎        | 155M/1.13G [00:20<02:09, 8.10MB/s] 13%|█▎        | 156M/1.13G [00:20<02:11, 8.01MB/s] 14%|█▎        | 157M/1.13G [00:21<02:08, 8.20MB/s] 14%|█▎        | 157M/1.13G [00:21<02:10, 8.07MB/s] 14%|█▎        | 158M/1.13G [00:21<02:07, 8.23MB/s] 14%|█▎        | 159M/1.13G [00:21<02:08, 8.17MB/s] 14%|█▍        | 160M/1.13G [00:21<02:07, 8.24MB/s] 14%|█▍        | 161M/1.13G [00:21<02:05, 8.37MB/s] 14%|█▍        | 162M/1.13G [00:21<02:08, 8.17MB/s] 14%|█▍        | 162M/1.13G [00:21<02:05, 8.32MB/s] 14%|█▍        | 163M/1.13G [00:21<02:07, 8.17MB/s] 14%|█▍        | 164M/1.13G [00:21<02:05, 8.29MB/s] 14%|█▍        | 165M/1.13G [00:22<02:03, 8.42MB/s] 14%|█▍        | 166M/1.13G [00:22<02:07, 8.19MB/s] 14%|█▍        | 166M/1.13G [00:22<02:06, 8.23MB/s] 14%|█▍        | 167M/1.13G [00:22<02:06, 8.22MB/s] 15%|█▍        | 168M/1.13G [00:22<02:04, 8.33MB/s] 15%|█▍        | 169M/1.13G [00:22<02:05, 8.25MB/s] 15%|█▍        | 170M/1.13G [00:22<02:05, 8.27MB/s] 15%|█▍        | 170M/1.13G [00:22<02:04, 8.29MB/s] 15%|█▍        | 171M/1.13G [00:22<02:05, 8.22MB/s] 15%|█▍        | 172M/1.13G [00:23<02:03, 8.37MB/s] 15%|█▍        | 173M/1.13G [00:23<02:06, 8.20MB/s] 15%|█▍        | 174M/1.13G [00:23<02:03, 8.33MB/s] 15%|█▌        | 175M/1.13G [00:23<02:02, 8.40MB/s] 15%|█▌        | 175M/1.13G [00:23<02:05, 8.21MB/s] 15%|█▌        | 176M/1.13G [00:23<02:03, 8.37MB/s] 15%|█▌        | 177M/1.13G [00:23<02:05, 8.19MB/s] 15%|█▌        | 178M/1.13G [00:23<02:03, 8.31MB/s] 15%|█▌        | 179M/1.13G [00:23<02:04, 8.28MB/s] 15%|█▌        | 179M/1.13G [00:23<02:04, 8.27MB/s] 16%|█▌        | 180M/1.13G [00:24<02:04, 8.25MB/s] 16%|█▌        | 181M/1.13G [00:24<02:05, 8.18MB/s] 16%|█▌        | 182M/1.13G [00:24<02:02, 8.39MB/s] 16%|█▌        | 183M/1.13G [00:24<02:04, 8.20MB/s] 16%|█▌        | 183M/1.13G [00:24<02:02, 8.34MB/s] 16%|█▌        | 184M/1.13G [00:24<02:02, 8.36MB/s] 16%|█▌        | 185M/1.13G [00:24<02:04, 8.20MB/s] 16%|█▌        | 186M/1.13G [00:25<03:34, 4.76MB/s] 16%|█▋        | 189M/1.13G [00:25<01:49, 9.29MB/s] 16%|█▋        | 190M/1.13G [00:25<01:49, 9.26MB/s] 16%|█▋        | 191M/1.13G [00:25<01:53, 8.91MB/s] 17%|█▋        | 192M/1.13G [00:25<01:56, 8.73MB/s] 17%|█▋        | 193M/1.13G [00:26<04:03, 4.16MB/s] 17%|█▋        | 194M/1.13G [00:26<03:32, 4.77MB/s] 17%|█▋        | 195M/1.13G [00:26<03:13, 5.23MB/s] 17%|█▋        | 195M/1.13G [00:26<02:51, 5.88MB/s] 17%|█▋        | 196M/1.13G [00:26<02:35, 6.48MB/s] 17%|█▋        | 197M/1.13G [00:26<02:29, 6.76MB/s] 17%|█▋        | 198M/1.13G [00:26<02:19, 7.24MB/s] 17%|█▋        | 199M/1.13G [00:26<02:16, 7.36MB/s] 17%|█▋        | 199M/1.13G [00:26<02:10, 7.72MB/s] 17%|█▋        | 200M/1.13G [00:27<02:09, 7.74MB/s] 17%|█▋        | 201M/1.13G [00:27<02:05, 7.98MB/s] 17%|█▋        | 202M/1.13G [00:27<02:02, 8.22MB/s] 17%|█▋        | 203M/1.13G [00:27<02:04, 8.04MB/s] 18%|█▊        | 204M/1.13G [00:27<02:01, 8.25MB/s] 18%|█▊        | 204M/1.13G [00:27<02:03, 8.08MB/s] 18%|█▊        | 205M/1.13G [00:27<02:00, 8.29MB/s] 18%|█▊        | 206M/1.13G [00:27<02:02, 8.14MB/s] 18%|█▊        | 207M/1.13G [00:27<02:00, 8.27MB/s] 18%|█▊        | 208M/1.13G [00:28<01:58, 8.40MB/s] 18%|█▊        | 209M/1.13G [00:28<02:01, 8.17MB/s] 18%|█▊        | 209M/1.13G [00:28<01:59, 8.34MB/s] 18%|█▊        | 210M/1.13G [00:28<02:02, 8.13MB/s] 18%|█▊        | 211M/1.13G [00:28<01:59, 8.29MB/s] 18%|█▊        | 212M/1.13G [00:28<01:57, 8.44MB/s] 18%|█▊        | 213M/1.13G [00:28<02:00, 8.22MB/s] 18%|█▊        | 213M/1.13G [00:28<01:58, 8.37MB/s] 18%|█▊        | 214M/1.13G [00:28<02:01, 8.16MB/s] 19%|█▊        | 215M/1.13G [00:28<01:59, 8.31MB/s] 19%|█▊        | 216M/1.13G [00:29<02:01, 8.15MB/s] 19%|█▊        | 217M/1.13G [00:29<01:58, 8.31MB/s] 19%|█▉        | 218M/1.13G [00:29<01:57, 8.43MB/s] 19%|█▉        | 218M/1.13G [00:29<02:00, 8.20MB/s] 19%|█▉        | 219M/1.13G [00:29<01:57, 8.38MB/s] 19%|█▉        | 220M/1.13G [00:29<02:01, 8.13MB/s] 19%|█▉        | 221M/1.13G [00:29<01:58, 8.33MB/s] 19%|█▉        | 222M/1.13G [00:29<01:59, 8.20MB/s] 19%|█▉        | 223M/1.13G [00:29<01:58, 8.28MB/s] 19%|█▉        | 223M/1.13G [00:29<01:56, 8.42MB/s] 19%|█▉        | 224M/1.13G [00:30<02:00, 8.16MB/s] 19%|█▉        | 225M/1.13G [00:30<01:56, 8.38MB/s] 19%|█▉        | 226M/1.13G [00:30<01:59, 8.17MB/s] 20%|█▉        | 227M/1.13G [00:30<01:58, 8.28MB/s] 20%|█▉        | 228M/1.13G [00:30<01:55, 8.48MB/s] 20%|█▉        | 228M/1.13G [00:30<01:58, 8.21MB/s] 20%|█▉        | 229M/1.13G [00:30<01:56, 8.39MB/s] 20%|█▉        | 230M/1.13G [00:30<01:59, 8.14MB/s] 20%|█▉        | 231M/1.13G [00:30<01:56, 8.32MB/s] 20%|█▉        | 232M/1.13G [00:31<01:59, 8.13MB/s] 20%|██        | 232M/1.13G [00:31<01:56, 8.31MB/s] 20%|██        | 233M/1.13G [00:31<01:54, 8.45MB/s] 20%|██        | 234M/1.13G [00:31<01:58, 8.20MB/s] 20%|██        | 235M/1.13G [00:31<01:56, 8.30MB/s] 20%|██        | 236M/1.13G [00:31<01:58, 8.15MB/s] 20%|██        | 237M/1.13G [00:31<01:56, 8.32MB/s] 20%|██        | 237M/1.13G [00:31<01:57, 8.23MB/s] 21%|██        | 238M/1.13G [00:31<01:56, 8.29MB/s] 21%|██        | 239M/1.13G [00:31<01:54, 8.43MB/s] 21%|██        | 240M/1.13G [00:32<01:57, 8.21MB/s] 21%|██        | 241M/1.13G [00:32<01:55, 8.36MB/s] 21%|██        | 241M/1.13G [00:32<01:57, 8.17MB/s] 21%|██        | 242M/1.13G [00:32<01:55, 8.35MB/s] 21%|██        | 243M/1.13G [00:32<01:53, 8.44MB/s] 21%|██        | 244M/1.13G [00:32<01:56, 8.21MB/s] 21%|██        | 245M/1.13G [00:32<01:55, 8.32MB/s] 21%|██        | 246M/1.13G [00:32<01:57, 8.15MB/s] 21%|██▏       | 246M/1.13G [00:32<01:57, 8.16MB/s] 21%|██▏       | 247M/1.13G [00:33<01:56, 8.20MB/s] 21%|██▏       | 248M/1.13G [00:33<01:54, 8.31MB/s] 21%|██▏       | 249M/1.13G [00:33<01:52, 8.45MB/s] 22%|██▏       | 250M/1.13G [00:33<01:56, 8.21MB/s] 22%|██▏       | 250M/1.13G [00:33<01:53, 8.38MB/s] 22%|██▏       | 251M/1.13G [00:33<01:56, 8.19MB/s] 22%|██▏       | 252M/1.13G [00:33<01:53, 8.35MB/s] 22%|██▏       | 253M/1.13G [00:33<01:53, 8.35MB/s] 22%|██▏       | 254M/1.13G [00:33<01:55, 8.22MB/s] 22%|██▏       | 255M/1.13G [00:33<01:53, 8.34MB/s] 22%|██▏       | 255M/1.13G [00:34<01:56, 8.16MB/s] 22%|██▏       | 256M/1.13G [00:34<01:53, 8.33MB/s] 22%|██▏       | 257M/1.13G [00:34<01:55, 8.19MB/s] 22%|██▏       | 258M/1.13G [00:34<01:53, 8.31MB/s] 22%|██▏       | 259M/1.13G [00:34<01:52, 8.41MB/s] 22%|██▏       | 259M/1.13G [00:34<01:55, 8.18MB/s] 22%|██▏       | 260M/1.13G [00:34<01:53, 8.33MB/s] 23%|██▎       | 261M/1.13G [00:34<01:55, 8.18MB/s] 23%|██▎       | 262M/1.13G [00:34<01:52, 8.33MB/s] 23%|██▎       | 263M/1.13G [00:34<01:53, 8.29MB/s] 23%|██▎       | 264M/1.13G [00:35<01:53, 8.25MB/s] 23%|██▎       | 264M/1.13G [00:35<01:51, 8.39MB/s] 23%|██▎       | 265M/1.13G [00:35<01:54, 8.18MB/s] 23%|██▎       | 266M/1.13G [00:35<01:52, 8.35MB/s] 23%|██▎       | 267M/1.13G [00:35<01:54, 8.19MB/s] 23%|██▎       | 268M/1.13G [00:35<01:52, 8.31MB/s] 23%|██▎       | 268M/1.13G [00:35<01:50, 8.42MB/s] 23%|██▎       | 269M/1.13G [00:35<01:54, 8.14MB/s] 23%|██▎       | 270M/1.13G [00:35<01:52, 8.32MB/s] 23%|██▎       | 271M/1.13G [00:36<01:53, 8.19MB/s] 23%|██▎       | 272M/1.13G [00:36<01:51, 8.35MB/s] 24%|██▎       | 273M/1.13G [00:36<01:52, 8.27MB/s] 24%|██▎       | 273M/1.13G [00:36<01:52, 8.27MB/s] 24%|██▎       | 274M/1.13G [00:36<01:50, 8.39MB/s] 24%|██▎       | 275M/1.13G [00:36<01:53, 8.18MB/s] 24%|██▍       | 276M/1.13G [00:36<01:50, 8.36MB/s] 24%|██▍       | 277M/1.13G [00:36<01:53, 8.18MB/s] 24%|██▍       | 278M/1.13G [00:36<01:50, 8.33MB/s] 24%|██▍       | 278M/1.13G [00:36<01:49, 8.42MB/s] 24%|██▍       | 279M/1.13G [00:37<01:52, 8.18MB/s] 24%|██▍       | 280M/1.13G [00:37<01:50, 8.32MB/s] 24%|██▍       | 281M/1.13G [00:37<01:53, 8.14MB/s] 24%|██▍       | 282M/1.13G [00:37<01:50, 8.34MB/s] 24%|██▍       | 282M/1.13G [00:37<01:52, 8.20MB/s] 24%|██▍       | 283M/1.13G [00:37<01:50, 8.31MB/s] 25%|██▍       | 284M/1.13G [00:37<01:49, 8.38MB/s] 25%|██▍       | 285M/1.13G [00:37<01:51, 8.21MB/s] 25%|██▍       | 286M/1.13G [00:37<01:49, 8.36MB/s] 25%|██▍       | 286M/1.13G [00:38<01:51, 8.17MB/s] 25%|██▍       | 287M/1.13G [00:38<01:49, 8.35MB/s] 25%|██▍       | 288M/1.13G [00:38<01:49, 8.36MB/s] 25%|██▍       | 289M/1.13G [00:38<01:50, 8.22MB/s] 25%|██▌       | 290M/1.13G [00:38<01:48, 8.37MB/s] 25%|██▌       | 291M/1.13G [00:38<01:52, 8.12MB/s] 25%|██▌       | 291M/1.13G [00:38<01:48, 8.36MB/s] 25%|██▌       | 292M/1.13G [00:38<01:50, 8.20MB/s] 25%|██▌       | 293M/1.13G [00:38<01:49, 8.31MB/s] 25%|██▌       | 294M/1.13G [00:38<01:47, 8.40MB/s] 25%|██▌       | 295M/1.13G [00:39<01:50, 8.19MB/s] 26%|██▌       | 296M/1.13G [00:39<01:48, 8.35MB/s] 26%|██▌       | 296M/1.13G [00:39<01:50, 8.18MB/s] 26%|██▌       | 297M/1.13G [00:39<01:48, 8.31MB/s] 26%|██▌       | 298M/1.13G [00:39<01:48, 8.32MB/s] 26%|██▌       | 299M/1.13G [00:39<01:49, 8.24MB/s] 26%|██▌       | 300M/1.13G [00:39<01:47, 8.38MB/s] 26%|██▌       | 300M/1.13G [00:39<01:50, 8.16MB/s] 26%|██▌       | 301M/1.13G [00:39<02:04, 7.25MB/s] 26%|██▌       | 302M/1.13G [00:40<01:45, 8.51MB/s] 26%|██▌       | 303M/1.13G [00:40<01:43, 8.64MB/s] 26%|██▌       | 304M/1.13G [00:40<01:47, 8.35MB/s] 26%|██▋       | 305M/1.13G [00:40<01:45, 8.46MB/s] 26%|██▋       | 306M/1.13G [00:40<01:45, 8.46MB/s] 26%|██▋       | 307M/1.13G [00:40<01:48, 8.23MB/s] 27%|██▋       | 307M/1.13G [00:40<01:46, 8.38MB/s] 27%|██▋       | 308M/1.13G [00:40<01:48, 8.21MB/s] 27%|██▋       | 309M/1.13G [00:40<01:47, 8.32MB/s] 27%|██▋       | 310M/1.13G [00:40<01:48, 8.23MB/s] 27%|██▋       | 311M/1.13G [00:41<01:46, 8.31MB/s] 27%|██▋       | 311M/1.13G [00:41<01:45, 8.39MB/s] 27%|██▋       | 312M/1.13G [00:41<01:48, 8.20MB/s] 27%|██▋       | 313M/1.13G [00:41<01:46, 8.34MB/s] 27%|██▋       | 314M/1.13G [00:41<01:47, 8.21MB/s] 27%|██▋       | 315M/1.13G [00:41<01:46, 8.33MB/s] 27%|██▋       | 316M/1.13G [00:41<01:45, 8.41MB/s] 27%|██▋       | 316M/1.13G [00:41<01:47, 8.19MB/s] 27%|██▋       | 317M/1.13G [00:42<03:01, 4.85MB/s] 27%|██▋       | 318M/1.13G [00:42<04:03, 3.62MB/s] 27%|██▋       | 319M/1.13G [00:42<03:18, 4.45MB/s] 28%|██▊       | 319M/1.13G [00:42<02:54, 5.04MB/s] 28%|██▊       | 320M/1.13G [00:42<02:30, 5.85MB/s] 28%|██▊       | 321M/1.13G [00:42<02:14, 6.52MB/s] 28%|██▊       | 322M/1.13G [00:42<02:09, 6.79MB/s] 28%|██▊       | 323M/1.13G [00:43<01:59, 7.32MB/s] 28%|██▊       | 323M/1.13G [00:43<01:58, 7.38MB/s] 28%|██▊       | 324M/1.13G [00:43<01:56, 7.49MB/s] 28%|██▊       | 325M/1.13G [00:43<01:51, 7.85MB/s] 28%|██▊       | 326M/1.13G [00:43<01:48, 8.07MB/s] 28%|██▊       | 327M/1.13G [00:43<01:45, 8.28MB/s] 28%|██▊       | 327M/1.13G [00:43<01:48, 8.07MB/s] 28%|██▊       | 328M/1.13G [00:44<03:15, 4.44MB/s] 29%|██▊       | 331M/1.13G [00:44<01:36, 8.98MB/s] 29%|██▊       | 332M/1.13G [00:44<01:30, 9.54MB/s] 29%|██▉       | 334M/1.13G [00:44<01:35, 9.05MB/s] 29%|██▉       | 335M/1.13G [00:44<01:36, 8.98MB/s] 29%|██▉       | 336M/1.13G [00:44<01:39, 8.67MB/s] 29%|██▉       | 337M/1.13G [00:44<01:39, 8.70MB/s] 29%|██▉       | 337M/1.13G [00:44<01:42, 8.43MB/s] 29%|██▉       | 338M/1.13G [00:45<01:40, 8.52MB/s] 29%|██▉       | 339M/1.13G [00:45<01:43, 8.30MB/s] 29%|██▉       | 340M/1.13G [00:45<01:42, 8.40MB/s] 29%|██▉       | 341M/1.13G [00:45<01:44, 8.19MB/s] 29%|██▉       | 342M/1.13G [00:45<01:42, 8.36MB/s] 30%|██▉       | 342M/1.13G [00:45<01:41, 8.45MB/s] 30%|██▉       | 343M/1.13G [00:45<01:43, 8.22MB/s] 30%|██▉       | 344M/1.13G [00:45<01:41, 8.38MB/s] 30%|██▉       | 345M/1.13G [00:45<01:44, 8.19MB/s] 30%|██▉       | 346M/1.13G [00:45<01:42, 8.30MB/s] 30%|██▉       | 347M/1.13G [00:46<01:43, 8.22MB/s] 30%|██▉       | 347M/1.13G [00:46<01:42, 8.27MB/s] 30%|███       | 348M/1.13G [00:46<01:40, 8.44MB/s] 30%|███       | 349M/1.13G [00:46<01:43, 8.18MB/s] 30%|███       | 350M/1.13G [00:46<01:41, 8.37MB/s] 30%|███       | 351M/1.13G [00:46<01:43, 8.17MB/s] 30%|███       | 351M/1.13G [00:46<01:42, 8.23MB/s] 30%|███       | 352M/1.13G [00:46<01:39, 8.46MB/s] 30%|███       | 353M/1.13G [00:46<01:42, 8.22MB/s] 31%|███       | 354M/1.13G [00:47<01:41, 8.34MB/s] 31%|███       | 355M/1.13G [00:47<01:43, 8.15MB/s] 31%|███       | 356M/1.13G [00:47<01:40, 8.37MB/s] 31%|███       | 356M/1.13G [00:47<01:43, 8.16MB/s] 31%|███       | 357M/1.13G [00:47<01:40, 8.33MB/s] 31%|███       | 358M/1.13G [00:47<01:39, 8.42MB/s] 31%|███       | 359M/1.13G [00:47<01:42, 8.17MB/s] 31%|███       | 360M/1.13G [00:47<01:40, 8.36MB/s] 31%|███       | 361M/1.13G [00:47<01:42, 8.19MB/s] 31%|███       | 361M/1.13G [00:47<01:40, 8.33MB/s] 31%|███▏      | 362M/1.13G [00:48<01:41, 8.21MB/s] 31%|███▏      | 363M/1.13G [00:48<01:40, 8.28MB/s] 31%|███▏      | 364M/1.13G [00:48<01:38, 8.42MB/s] 31%|███▏      | 365M/1.13G [00:48<01:41, 8.17MB/s] 32%|███▏      | 365M/1.13G [00:48<01:39, 8.35MB/s] 32%|███▏      | 366M/1.13G [00:48<01:42, 8.13MB/s] 32%|███▏      | 367M/1.13G [00:48<01:39, 8.32MB/s] 32%|███▏      | 368M/1.13G [00:48<01:38, 8.43MB/s] 32%|███▏      | 369M/1.13G [00:48<01:41, 8.20MB/s] 32%|███▏      | 370M/1.13G [00:49<01:46, 7.74MB/s] 32%|███▏      | 371M/1.13G [00:49<01:38, 8.36MB/s] 32%|███▏      | 371M/1.13G [00:49<01:37, 8.51MB/s] 32%|███▏      | 372M/1.13G [00:49<01:40, 8.23MB/s] 32%|███▏      | 373M/1.13G [00:49<01:37, 8.41MB/s] 32%|███▏      | 374M/1.13G [00:49<01:37, 8.46MB/s] 32%|███▏      | 375M/1.13G [00:49<01:40, 8.22MB/s] 32%|███▏      | 376M/1.13G [00:49<01:37, 8.39MB/s] 32%|███▏      | 376M/1.13G [00:49<01:40, 8.17MB/s] 33%|███▎      | 377M/1.13G [00:49<01:37, 8.40MB/s] 33%|███▎      | 378M/1.13G [00:50<01:40, 8.15MB/s] 33%|███▎      | 379M/1.13G [00:50<01:38, 8.30MB/s] 33%|███▎      | 380M/1.13G [00:50<01:36, 8.46MB/s] 33%|███▎      | 380M/1.13G [00:50<01:39, 8.17MB/s] 33%|███▎      | 381M/1.13G [00:50<01:37, 8.34MB/s] 33%|███▎      | 382M/1.13G [00:50<02:38, 5.14MB/s] 33%|███▎      | 384M/1.13G [00:51<02:20, 5.80MB/s] 33%|███▎      | 385M/1.13G [00:51<02:07, 6.34MB/s] 33%|███▎      | 385M/1.13G [00:51<01:58, 6.86MB/s] 33%|███▎      | 386M/1.13G [00:51<01:55, 7.02MB/s] 33%|███▎      | 387M/1.13G [00:51<01:48, 7.46MB/s] 33%|███▎      | 388M/1.13G [00:51<01:47, 7.50MB/s] 34%|███▎      | 389M/1.13G [00:51<01:43, 7.82MB/s] 34%|███▎      | 389M/1.13G [00:51<01:39, 8.09MB/s] 34%|███▎      | 390M/1.13G [00:51<01:41, 7.96MB/s] 34%|███▍      | 391M/1.13G [00:51<01:38, 8.17MB/s] 34%|███▍      | 392M/1.13G [00:52<01:40, 8.02MB/s] 34%|███▍      | 393M/1.13G [00:52<01:37, 8.22MB/s] 34%|███▍      | 393M/1.13G [00:52<01:39, 8.10MB/s] 34%|███▍      | 394M/1.13G [00:52<01:37, 8.23MB/s] 34%|███▍      | 395M/1.13G [00:52<01:35, 8.40MB/s] 34%|███▍      | 396M/1.13G [00:52<01:37, 8.18MB/s] 34%|███▍      | 397M/1.13G [00:52<01:35, 8.34MB/s] 34%|███▍      | 398M/1.13G [00:52<01:38, 8.12MB/s] 34%|███▍      | 398M/1.13G [00:52<01:35, 8.32MB/s] 34%|███▍      | 399M/1.13G [00:53<01:34, 8.41MB/s] 35%|███▍      | 400M/1.13G [00:53<01:36, 8.20MB/s] 35%|███▍      | 401M/1.13G [00:53<01:35, 8.34MB/s] 35%|███▍      | 402M/1.13G [00:53<01:37, 8.16MB/s] 35%|███▍      | 403M/1.13G [00:53<01:35, 8.33MB/s] 35%|███▍      | 403M/1.13G [00:53<01:37, 8.14MB/s] 35%|███▍      | 404M/1.13G [00:53<01:35, 8.31MB/s] 35%|███▍      | 405M/1.13G [00:53<01:34, 8.37MB/s] 35%|███▌      | 406M/1.13G [00:53<01:36, 8.21MB/s] 35%|███▌      | 407M/1.13G [00:53<01:34, 8.37MB/s] 35%|███▌      | 407M/1.13G [00:54<01:36, 8.17MB/s] 35%|███▌      | 408M/1.13G [00:54<01:34, 8.33MB/s] 35%|███▌      | 409M/1.13G [00:54<01:34, 8.33MB/s] 35%|███▌      | 410M/1.13G [00:54<01:35, 8.26MB/s] 35%|███▌      | 411M/1.13G [00:54<01:33, 8.39MB/s] 36%|███▌      | 412M/1.13G [00:54<01:35, 8.19MB/s] 36%|███▌      | 412M/1.13G [00:54<01:33, 8.33MB/s] 36%|███▌      | 413M/1.13G [00:54<01:36, 8.13MB/s] 36%|███▌      | 414M/1.13G [00:54<01:34, 8.30MB/s] 36%|███▌      | 415M/1.13G [00:54<01:32, 8.46MB/s] 36%|███▌      | 416M/1.13G [00:55<01:34, 8.22MB/s] 36%|███▌      | 417M/1.13G [00:55<01:33, 8.36MB/s] 36%|███▌      | 417M/1.13G [00:55<01:35, 8.16MB/s] 36%|███▌      | 418M/1.13G [00:55<01:33, 8.32MB/s] 36%|███▌      | 419M/1.13G [00:55<01:35, 8.12MB/s] 36%|███▌      | 420M/1.13G [00:55<01:33, 8.32MB/s] 36%|███▋      | 421M/1.13G [00:55<01:32, 8.41MB/s] 36%|███▋      | 421M/1.13G [00:55<01:34, 8.18MB/s] 36%|███▋      | 422M/1.13G [00:55<01:32, 8.36MB/s] 37%|███▋      | 423M/1.13G [00:56<03:27, 3.72MB/s] 37%|███▋      | 427M/1.13G [00:56<01:17, 9.87MB/s] 37%|███▋      | 429M/1.13G [00:56<01:21, 9.34MB/s] 37%|███▋      | 430M/1.13G [00:56<01:23, 9.19MB/s] 37%|███▋      | 431M/1.13G [00:57<01:25, 8.90MB/s] 37%|███▋      | 433M/1.13G [00:57<01:26, 8.84MB/s] 37%|███▋      | 434M/1.13G [00:57<01:28, 8.60MB/s] 38%|███▊      | 435M/1.13G [00:57<01:28, 8.62MB/s] 38%|███▊      | 435M/1.13G [00:57<01:30, 8.41MB/s] 38%|███▊      | 436M/1.13G [00:57<01:29, 8.49MB/s] 38%|███▊      | 437M/1.13G [00:57<01:31, 8.26MB/s] 38%|███▊      | 438M/1.13G [00:58<02:39, 4.73MB/s] 38%|███▊      | 441M/1.13G [00:58<01:23, 8.98MB/s] 38%|███▊      | 442M/1.13G [00:58<01:18, 9.55MB/s] 38%|███▊      | 443M/1.13G [00:58<01:22, 9.07MB/s] 38%|███▊      | 444M/1.13G [00:58<01:24, 8.91MB/s] 38%|███▊      | 445M/1.13G [00:58<01:25, 8.74MB/s] 39%|███▊      | 446M/1.13G [00:58<01:26, 8.63MB/s] 39%|███▊      | 447M/1.13G [00:59<01:27, 8.48MB/s] 39%|███▊      | 448M/1.13G [00:59<02:09, 5.76MB/s] 39%|███▉      | 449M/1.13G [00:59<02:05, 5.93MB/s] 39%|███▉      | 450M/1.13G [00:59<01:55, 6.43MB/s] 39%|███▉      | 451M/1.13G [00:59<01:47, 6.90MB/s] 39%|███▉      | 452M/1.13G [00:59<01:44, 7.06MB/s] 39%|███▉      | 453M/1.13G [01:00<01:39, 7.45MB/s] 39%|███▉      | 453M/1.13G [01:00<01:38, 7.53MB/s] 39%|███▉      | 454M/1.13G [01:00<01:34, 7.83MB/s] 39%|███▉      | 455M/1.13G [01:00<01:31, 8.07MB/s] 39%|███▉      | 456M/1.13G [01:00<01:32, 7.97MB/s] 39%|███▉      | 457M/1.13G [01:00<01:30, 8.16MB/s] 39%|███▉      | 458M/1.13G [01:00<01:31, 8.03MB/s] 40%|███▉      | 458M/1.13G [01:00<01:29, 8.22MB/s] 40%|███▉      | 459M/1.13G [01:00<01:27, 8.35MB/s] 40%|███▉      | 460M/1.13G [01:00<01:29, 8.16MB/s] 40%|███▉      | 461M/1.13G [01:01<01:29, 8.15MB/s] 40%|███▉      | 462M/1.13G [01:01<01:29, 8.18MB/s] 40%|███▉      | 462M/1.13G [01:01<01:27, 8.34MB/s] 40%|███▉      | 463M/1.13G [01:01<01:28, 8.20MB/s] 40%|████      | 464M/1.13G [01:01<01:27, 8.29MB/s] 40%|████      | 465M/1.13G [01:01<01:26, 8.41MB/s] 40%|████      | 466M/1.13G [01:01<01:28, 8.19MB/s] 40%|████      | 467M/1.13G [01:01<01:27, 8.34MB/s] 40%|████      | 467M/1.13G [01:01<01:28, 8.21MB/s] 40%|████      | 468M/1.13G [01:01<01:26, 8.33MB/s] 40%|████      | 469M/1.13G [01:02<01:26, 8.39MB/s] 41%|████      | 470M/1.13G [01:02<01:28, 8.20MB/s] 41%|████      | 471M/1.13G [01:02<01:26, 8.36MB/s] 41%|████      | 471M/1.13G [01:02<01:28, 8.18MB/s] 41%|████      | 472M/1.13G [01:02<01:26, 8.31MB/s] 41%|████      | 473M/1.13G [01:02<01:27, 8.17MB/s] 41%|████      | 474M/1.13G [01:02<01:26, 8.29MB/s] 41%|████      | 475M/1.13G [01:02<01:25, 8.43MB/s] 41%|████      | 476M/1.13G [01:02<01:27, 8.20MB/s] 41%|████      | 476M/1.13G [01:03<01:25, 8.33MB/s] 41%|████      | 477M/1.13G [01:03<01:27, 8.16MB/s] 41%|████▏     | 478M/1.13G [01:03<01:25, 8.30MB/s] 41%|████▏     | 479M/1.13G [01:03<01:26, 8.26MB/s] 41%|████▏     | 480M/1.13G [01:03<01:26, 8.25MB/s] 41%|████▏     | 480M/1.13G [01:03<01:24, 8.39MB/s] 42%|████▏     | 481M/1.13G [01:03<01:26, 8.17MB/s] 42%|████▏     | 482M/1.13G [01:03<01:25, 8.32MB/s] 42%|████▏     | 483M/1.13G [01:03<01:26, 8.21MB/s] 42%|████▏     | 484M/1.13G [01:03<01:24, 8.34MB/s] 42%|████▏     | 485M/1.13G [01:04<01:24, 8.39MB/s] 42%|████▏     | 485M/1.13G [01:04<01:25, 8.22MB/s] 42%|████▏     | 486M/1.13G [01:04<01:24, 8.33MB/s] 42%|████▏     | 487M/1.13G [01:04<01:26, 8.18MB/s] 42%|████▏     | 488M/1.13G [01:04<01:24, 8.32MB/s] 42%|████▏     | 489M/1.13G [01:04<01:24, 8.33MB/s] 42%|████▏     | 489M/1.13G [01:04<01:25, 8.22MB/s] 42%|████▏     | 490M/1.13G [01:04<01:23, 8.38MB/s] 42%|████▏     | 491M/1.13G [01:04<01:25, 8.18MB/s] 42%|████▏     | 492M/1.13G [01:05<02:20, 4.98MB/s] 43%|████▎     | 494M/1.13G [01:05<01:57, 5.95MB/s] 43%|████▎     | 494M/1.13G [01:05<01:47, 6.46MB/s] 43%|████▎     | 495M/1.13G [01:05<01:40, 6.95MB/s] 43%|████▎     | 496M/1.13G [01:05<01:38, 7.08MB/s] 43%|████▎     | 497M/1.13G [01:05<01:32, 7.47MB/s] 43%|████▎     | 498M/1.13G [01:05<01:31, 7.54MB/s] 43%|████▎     | 498M/1.13G [01:06<01:28, 7.82MB/s] 43%|████▎     | 499M/1.13G [01:06<01:25, 8.06MB/s] 43%|████▎     | 500M/1.13G [01:06<01:26, 7.97MB/s] 43%|████▎     | 501M/1.13G [01:06<01:24, 8.15MB/s] 43%|████▎     | 502M/1.13G [01:06<01:25, 8.05MB/s] 43%|████▎     | 502M/1.13G [01:06<01:23, 8.20MB/s] 43%|████▎     | 503M/1.13G [01:06<01:22, 8.35MB/s] 44%|████▎     | 504M/1.13G [01:06<01:23, 8.19MB/s] 44%|████▎     | 505M/1.13G [01:06<01:22, 8.28MB/s] 44%|████▎     | 506M/1.13G [01:06<01:24, 8.15MB/s] 44%|████▎     | 507M/1.13G [01:07<01:22, 8.28MB/s] 44%|████▍     | 507M/1.13G [01:07<01:24, 8.06MB/s] 44%|████▍     | 508M/1.13G [01:07<01:22, 8.31MB/s] 44%|████▍     | 509M/1.13G [01:07<01:21, 8.39MB/s] 44%|████▍     | 510M/1.13G [01:07<01:22, 8.25MB/s] 44%|████▍     | 511M/1.13G [01:07<01:21, 8.35MB/s] 44%|████▍     | 511M/1.13G [01:07<01:22, 8.20MB/s] 44%|████▍     | 512M/1.13G [01:07<01:21, 8.31MB/s] 44%|████▍     | 513M/1.13G [01:07<01:20, 8.44MB/s] 44%|████▍     | 514M/1.13G [01:08<01:22, 8.22MB/s] 44%|████▍     | 515M/1.13G [01:08<01:21, 8.33MB/s] 44%|████▍     | 516M/1.13G [01:08<01:22, 8.20MB/s] 45%|████▍     | 516M/1.13G [01:08<01:21, 8.29MB/s] 45%|████▍     | 517M/1.13G [01:08<01:22, 8.20MB/s] 45%|████▍     | 518M/1.13G [01:08<01:21, 8.26MB/s] 45%|████▍     | 519M/1.13G [01:08<01:19, 8.41MB/s] 45%|████▍     | 520M/1.13G [01:08<01:21, 8.23MB/s] 45%|████▍     | 520M/1.13G [01:08<01:20, 8.32MB/s] 45%|████▍     | 521M/1.13G [01:08<01:21, 8.19MB/s] 45%|████▌     | 522M/1.13G [01:09<01:20, 8.25MB/s] 45%|████▌     | 523M/1.13G [01:09<01:18, 8.45MB/s] 45%|████▌     | 524M/1.13G [01:09<01:20, 8.24MB/s] 45%|████▌     | 525M/1.13G [01:09<01:19, 8.32MB/s] 45%|████▌     | 525M/1.13G [01:09<01:20, 8.21MB/s] 45%|████▌     | 526M/1.13G [01:09<01:20, 8.28MB/s] 45%|████▌     | 527M/1.13G [01:09<01:20, 8.21MB/s] 46%|████▌     | 528M/1.13G [01:09<01:19, 8.27MB/s] 46%|████▌     | 529M/1.13G [01:09<01:18, 8.40MB/s] 46%|████▌     | 529M/1.13G [01:10<02:13, 4.94MB/s] 46%|████▌     | 531M/1.13G [01:10<01:50, 5.94MB/s] 46%|████▌     | 532M/1.13G [01:10<02:31, 4.35MB/s] 46%|████▌     | 533M/1.13G [01:10<02:10, 5.05MB/s] 46%|████▌     | 533M/1.13G [01:10<01:59, 5.48MB/s] 46%|████▌     | 534M/1.13G [01:11<01:46, 6.16MB/s] 46%|████▌     | 535M/1.13G [01:11<01:36, 6.77MB/s] 46%|████▌     | 536M/1.13G [01:11<01:33, 6.96MB/s] 46%|████▋     | 537M/1.13G [01:11<01:27, 7.45MB/s] 46%|████▋     | 537M/1.13G [01:11<01:26, 7.49MB/s] 46%|████▋     | 538M/1.13G [01:11<01:23, 7.83MB/s] 47%|████▋     | 539M/1.13G [01:11<01:23, 7.77MB/s] 47%|████▋     | 540M/1.13G [01:11<01:20, 8.06MB/s] 47%|████▋     | 541M/1.13G [01:12<02:12, 4.90MB/s] 47%|████▋     | 542M/1.13G [01:12<01:49, 5.91MB/s] 47%|████▋     | 543M/1.13G [01:12<01:40, 6.40MB/s] 47%|████▋     | 544M/1.13G [01:12<01:33, 6.87MB/s] 47%|████▋     | 545M/1.13G [01:12<01:31, 7.06MB/s] 47%|████▋     | 546M/1.13G [01:12<01:26, 7.44MB/s] 47%|████▋     | 546M/1.13G [01:12<01:25, 7.53MB/s] 47%|████▋     | 547M/1.13G [01:12<01:22, 7.81MB/s] 47%|████▋     | 548M/1.13G [01:13<01:19, 8.05MB/s] 47%|████▋     | 549M/1.13G [01:13<01:20, 7.97MB/s] 47%|████▋     | 550M/1.13G [01:13<02:15, 4.73MB/s] 48%|████▊     | 552M/1.13G [01:13<01:46, 5.97MB/s] 48%|████▊     | 553M/1.13G [01:13<01:33, 6.77MB/s] 48%|████▊     | 554M/1.13G [01:14<01:33, 6.79MB/s] 48%|████▊     | 554M/1.13G [01:14<01:24, 7.53MB/s] 48%|████▊     | 555M/1.13G [01:14<01:26, 7.34MB/s] 48%|████▊     | 556M/1.13G [01:14<01:18, 8.00MB/s] 48%|████▊     | 557M/1.13G [01:14<01:22, 7.66MB/s] 48%|████▊     | 558M/1.13G [01:14<01:16, 8.26MB/s] 48%|████▊     | 559M/1.13G [01:14<01:20, 7.84MB/s] 48%|████▊     | 560M/1.13G [01:14<01:14, 8.42MB/s] 48%|████▊     | 561M/1.13G [01:15<01:18, 7.93MB/s] 49%|████▊     | 562M/1.13G [01:15<01:13, 8.48MB/s] 49%|████▊     | 563M/1.13G [01:15<01:18, 8.00MB/s] 49%|████▊     | 564M/1.13G [01:15<01:13, 8.50MB/s] 49%|████▉     | 565M/1.13G [01:15<01:09, 8.98MB/s] 49%|████▉     | 566M/1.13G [01:15<01:15, 8.19MB/s] 49%|████▉     | 567M/1.13G [01:15<01:10, 8.74MB/s] 49%|████▉     | 568M/1.13G [01:15<01:17, 8.02MB/s] 49%|████▉     | 569M/1.13G [01:15<01:11, 8.63MB/s] 49%|████▉     | 570M/1.13G [01:16<01:17, 7.94MB/s] 49%|████▉     | 571M/1.13G [01:16<01:11, 8.57MB/s] 49%|████▉     | 572M/1.13G [01:16<01:17, 7.90MB/s] 49%|████▉     | 573M/1.13G [01:16<01:12, 8.53MB/s] 49%|████▉     | 573M/1.13G [01:16<01:17, 7.87MB/s] 50%|████▉     | 574M/1.13G [01:16<01:12, 8.50MB/s] 50%|████▉     | 575M/1.13G [01:16<01:17, 7.85MB/s] 50%|████▉     | 576M/1.13G [01:16<01:11, 8.49MB/s] 50%|████▉     | 577M/1.13G [01:17<01:17, 7.87MB/s] 50%|████▉     | 578M/1.13G [01:17<01:11, 8.49MB/s] 50%|████▉     | 579M/1.13G [01:17<01:16, 7.92MB/s] 50%|█████     | 580M/1.13G [01:17<01:11, 8.44MB/s] 50%|█████     | 581M/1.13G [01:17<01:15, 8.07MB/s] 50%|█████     | 582M/1.13G [01:17<01:11, 8.41MB/s] 50%|█████     | 583M/1.13G [01:17<01:07, 8.94MB/s] 50%|█████     | 584M/1.13G [01:17<01:14, 8.13MB/s] 50%|█████     | 585M/1.13G [01:17<01:09, 8.71MB/s] 51%|█████     | 586M/1.13G [01:18<01:15, 8.00MB/s] 51%|█████     | 586M/1.13G [01:18<01:09, 8.61MB/s] 51%|█████     | 587M/1.13G [01:18<01:15, 7.94MB/s] 51%|█████     | 588M/1.13G [01:18<01:09, 8.56MB/s] 51%|█████     | 589M/1.13G [01:18<01:15, 7.87MB/s] 51%|█████     | 590M/1.13G [01:18<01:09, 8.53MB/s] 51%|█████     | 591M/1.13G [01:18<01:15, 7.88MB/s] 51%|█████     | 592M/1.13G [01:18<01:09, 8.49MB/s] 51%|█████     | 593M/1.13G [01:19<01:15, 7.88MB/s] 51%|█████▏    | 594M/1.13G [01:19<01:09, 8.50MB/s] 51%|█████▏    | 595M/1.13G [01:19<01:14, 7.92MB/s] 51%|█████▏    | 596M/1.13G [01:19<01:09, 8.44MB/s] 51%|█████▏    | 597M/1.13G [01:19<01:12, 8.10MB/s] 52%|█████▏    | 597M/1.13G [01:19<01:10, 8.36MB/s] 52%|█████▏    | 598M/1.13G [01:19<01:05, 8.91MB/s] 52%|█████▏    | 599M/1.13G [01:19<01:12, 8.09MB/s] 52%|█████▏    | 600M/1.13G [01:19<01:07, 8.72MB/s] 52%|█████▏    | 601M/1.13G [01:20<01:13, 7.96MB/s] 52%|█████▏    | 602M/1.13G [01:20<01:07, 8.61MB/s] 52%|█████▏    | 603M/1.13G [01:20<01:13, 7.92MB/s] 52%|█████▏    | 604M/1.13G [01:20<01:07, 8.57MB/s] 52%|█████▏    | 605M/1.13G [01:20<01:13, 7.91MB/s] 52%|█████▏    | 606M/1.13G [01:20<01:08, 8.52MB/s] 52%|█████▏    | 607M/1.13G [01:20<01:13, 7.89MB/s] 52%|█████▏    | 608M/1.13G [01:20<01:07, 8.50MB/s] 53%|█████▎    | 608M/1.13G [01:20<01:13, 7.87MB/s] 53%|█████▎    | 609M/1.13G [01:21<01:08, 8.47MB/s] 53%|█████▎    | 610M/1.13G [01:21<01:11, 8.00MB/s] 53%|█████▎    | 611M/1.13G [01:21<01:08, 8.35MB/s] 53%|█████▎    | 612M/1.13G [01:21<01:04, 8.91MB/s] 53%|█████▎    | 613M/1.13G [01:21<01:10, 8.08MB/s] 53%|█████▎    | 614M/1.13G [01:22<05:10, 1.84MB/s] 54%|█████▎    | 620M/1.13G [01:23<01:29, 6.28MB/s] 54%|█████▎    | 621M/1.13G [01:23<01:26, 6.55MB/s] 54%|█████▎    | 622M/1.13G [01:23<01:22, 6.80MB/s] 54%|█████▍    | 623M/1.13G [01:23<01:19, 7.04MB/s] 54%|█████▍    | 624M/1.13G [01:23<01:17, 7.26MB/s] 54%|█████▍    | 625M/1.13G [01:23<01:14, 7.46MB/s] 54%|█████▍    | 626M/1.13G [01:23<01:13, 7.63MB/s] 54%|█████▍    | 627M/1.13G [01:24<01:11, 7.79MB/s] 54%|█████▍    | 628M/1.13G [01:24<01:10, 7.94MB/s] 54%|█████▍    | 629M/1.13G [01:24<01:09, 7.99MB/s] 54%|█████▍    | 629M/1.13G [01:24<01:08, 8.06MB/s] 54%|█████▍    | 630M/1.13G [01:24<01:08, 8.12MB/s] 54%|█████▍    | 631M/1.13G [01:24<01:07, 8.17MB/s] 55%|█████▍    | 632M/1.13G [01:24<01:06, 8.35MB/s] 55%|█████▍    | 633M/1.13G [01:24<01:07, 8.18MB/s] 55%|█████▍    | 633M/1.13G [01:24<01:07, 8.18MB/s] 55%|█████▍    | 634M/1.13G [01:24<01:06, 8.22MB/s] 55%|█████▍    | 635M/1.13G [01:25<01:06, 8.26MB/s] 55%|█████▍    | 636M/1.13G [01:25<01:06, 8.27MB/s] 55%|█████▍    | 637M/1.13G [01:25<01:06, 8.29MB/s] 55%|█████▌    | 637M/1.13G [01:25<01:06, 8.20MB/s] 55%|█████▌    | 638M/1.13G [01:25<01:05, 8.30MB/s] 55%|█████▌    | 639M/1.13G [01:25<01:05, 8.28MB/s] 55%|█████▌    | 640M/1.13G [01:25<01:05, 8.29MB/s] 55%|█████▌    | 641M/1.13G [01:25<01:05, 8.29MB/s] 55%|█████▌    | 641M/1.13G [01:25<01:05, 8.27MB/s] 55%|█████▌    | 642M/1.13G [01:25<01:05, 8.26MB/s] 56%|█████▌    | 643M/1.13G [01:26<01:05, 8.28MB/s] 56%|█████▌    | 644M/1.13G [01:26<01:05, 8.29MB/s] 56%|█████▌    | 645M/1.13G [01:26<01:05, 8.29MB/s] 56%|█████▌    | 645M/1.13G [01:26<01:04, 8.34MB/s] 56%|█████▌    | 646M/1.13G [01:26<01:05, 8.26MB/s] 56%|█████▌    | 647M/1.13G [01:26<01:04, 8.26MB/s] 56%|█████▌    | 648M/1.13G [01:27<02:11, 4.09MB/s] 56%|█████▌    | 651M/1.13G [01:27<00:54, 9.81MB/s] 56%|█████▋    | 653M/1.13G [01:27<00:57, 9.27MB/s] 56%|█████▋    | 654M/1.13G [01:27<00:58, 9.00MB/s] 57%|█████▋    | 655M/1.13G [01:27<00:59, 8.84MB/s] 57%|█████▋    | 656M/1.13G [01:27<01:00, 8.66MB/s] 57%|█████▋    | 657M/1.13G [01:27<01:00, 8.62MB/s] 57%|█████▋    | 658M/1.13G [01:27<01:01, 8.48MB/s] 57%|█████▋    | 659M/1.13G [01:28<01:02, 8.38MB/s] 57%|█████▋    | 660M/1.13G [01:28<01:02, 8.39MB/s] 57%|█████▋    | 661M/1.13G [01:28<01:02, 8.38MB/s] 57%|█████▋    | 662M/1.13G [01:28<01:02, 8.35MB/s] 57%|█████▋    | 662M/1.13G [01:28<01:02, 8.33MB/s] 57%|█████▋    | 663M/1.13G [01:28<01:01, 8.38MB/s] 57%|█████▋    | 664M/1.13G [01:28<01:02, 8.27MB/s] 57%|█████▋    | 665M/1.13G [01:28<01:02, 8.28MB/s] 57%|█████▋    | 666M/1.13G [01:28<01:02, 8.27MB/s] 58%|█████▊    | 666M/1.13G [01:29<01:02, 8.24MB/s] 58%|█████▊    | 667M/1.13G [01:29<01:02, 8.30MB/s] 58%|█████▊    | 668M/1.13G [01:29<01:01, 8.30MB/s] 58%|█████▊    | 669M/1.13G [01:29<01:02, 8.24MB/s] 58%|█████▊    | 670M/1.13G [01:29<01:01, 8.27MB/s] 58%|█████▊    | 670M/1.13G [01:29<01:01, 8.30MB/s] 58%|█████▊    | 671M/1.13G [01:29<01:01, 8.29MB/s] 58%|█████▊    | 672M/1.13G [01:29<01:01, 8.28MB/s] 58%|█████▊    | 673M/1.13G [01:29<01:01, 8.25MB/s] 58%|█████▊    | 674M/1.13G [01:29<01:01, 8.28MB/s] 58%|█████▊    | 674M/1.13G [01:30<01:01, 8.28MB/s] 58%|█████▊    | 675M/1.13G [01:30<01:01, 8.30MB/s] 58%|█████▊    | 676M/1.13G [01:30<01:01, 8.29MB/s] 58%|█████▊    | 677M/1.13G [01:30<01:00, 8.30MB/s] 58%|█████▊    | 678M/1.13G [01:30<01:01, 8.27MB/s] 59%|█████▊    | 678M/1.13G [01:30<01:00, 8.29MB/s] 59%|█████▊    | 679M/1.13G [01:30<01:00, 8.30MB/s] 59%|█████▊    | 680M/1.13G [01:30<01:00, 8.28MB/s] 59%|█████▉    | 681M/1.13G [01:30<01:00, 8.34MB/s] 59%|█████▉    | 682M/1.13G [01:30<01:00, 8.24MB/s] 59%|█████▉    | 682M/1.13G [01:31<01:01, 8.15MB/s] 59%|█████▉    | 683M/1.13G [01:31<00:59, 8.32MB/s] 59%|█████▉    | 684M/1.13G [01:31<01:00, 8.26MB/s] 59%|█████▉    | 685M/1.13G [01:31<00:59, 8.35MB/s] 59%|█████▉    | 686M/1.13G [01:31<00:59, 8.30MB/s] 59%|█████▉    | 686M/1.13G [01:31<01:00, 8.12MB/s] 59%|█████▉    | 687M/1.13G [01:31<00:59, 8.31MB/s] 59%|█████▉    | 688M/1.13G [01:31<00:59, 8.31MB/s] 59%|█████▉    | 689M/1.13G [01:31<00:59, 8.31MB/s] 60%|█████▉    | 690M/1.13G [01:31<00:59, 8.31MB/s] 60%|█████▉    | 690M/1.13G [01:32<00:59, 8.21MB/s] 60%|█████▉    | 691M/1.13G [01:32<02:06, 3.87MB/s] 60%|█████▉    | 695M/1.13G [01:32<00:49, 9.85MB/s] 60%|██████    | 697M/1.13G [01:32<00:51, 9.37MB/s] 60%|██████    | 698M/1.13G [01:33<00:54, 8.91MB/s] 60%|██████    | 699M/1.13G [01:33<00:54, 8.90MB/s] 60%|██████    | 700M/1.13G [01:33<00:55, 8.68MB/s] 61%|██████    | 701M/1.13G [01:33<00:55, 8.66MB/s] 61%|██████    | 702M/1.13G [01:33<00:56, 8.45MB/s] 61%|██████    | 703M/1.13G [01:33<00:55, 8.54MB/s] 61%|██████    | 704M/1.13G [01:33<00:57, 8.28MB/s] 61%|██████    | 705M/1.13G [01:33<00:56, 8.50MB/s] 61%|██████    | 706M/1.13G [01:33<00:56, 8.39MB/s] 61%|██████    | 706M/1.13G [01:34<00:56, 8.43MB/s] 61%|██████    | 707M/1.13G [01:34<00:56, 8.36MB/s] 61%|██████    | 708M/1.13G [01:34<00:57, 8.19MB/s] 61%|██████    | 709M/1.13G [01:34<00:56, 8.35MB/s] 61%|██████    | 710M/1.13G [01:34<00:56, 8.27MB/s] 61%|██████▏   | 710M/1.13G [01:34<00:56, 8.31MB/s] 61%|██████▏   | 711M/1.13G [01:34<00:56, 8.33MB/s] 61%|██████▏   | 712M/1.13G [01:34<00:56, 8.25MB/s] 62%|██████▏   | 713M/1.13G [01:34<00:56, 8.29MB/s] 62%|██████▏   | 714M/1.13G [01:35<00:57, 8.08MB/s] 62%|██████▏   | 714M/1.13G [01:35<01:29, 5.22MB/s] 62%|██████▏   | 716M/1.13G [01:35<01:22, 5.61MB/s] 62%|██████▏   | 717M/1.13G [01:35<01:14, 6.20MB/s] 62%|██████▏   | 717M/1.13G [01:35<01:08, 6.74MB/s] 62%|██████▏   | 718M/1.13G [01:35<01:06, 6.94MB/s] 62%|██████▏   | 719M/1.13G [01:35<01:02, 7.38MB/s] 62%|██████▏   | 720M/1.13G [01:36<01:01, 7.48MB/s] 62%|██████▏   | 721M/1.13G [01:36<00:58, 7.79MB/s] 62%|██████▏   | 721M/1.13G [01:36<00:57, 8.03MB/s] 62%|██████▏   | 722M/1.13G [01:36<00:57, 7.95MB/s] 62%|██████▏   | 723M/1.13G [01:36<00:55, 8.16MB/s] 62%|██████▏   | 724M/1.13G [01:36<00:56, 8.05MB/s] 63%|██████▎   | 725M/1.13G [01:36<00:55, 8.20MB/s] 63%|██████▎   | 726M/1.13G [01:36<00:54, 8.34MB/s] 63%|██████▎   | 726M/1.13G [01:36<00:55, 8.15MB/s] 63%|██████▎   | 727M/1.13G [01:36<00:54, 8.31MB/s] 63%|██████▎   | 728M/1.13G [01:37<00:55, 8.18MB/s] 63%|██████▎   | 729M/1.13G [01:37<01:08, 6.62MB/s] 63%|██████▎   | 730M/1.13G [01:37<00:51, 8.75MB/s] 63%|██████▎   | 731M/1.13G [01:37<00:51, 8.74MB/s] 63%|██████▎   | 732M/1.13G [01:37<00:52, 8.45MB/s] 63%|██████▎   | 733M/1.13G [01:37<00:52, 8.52MB/s] 63%|██████▎   | 734M/1.13G [01:37<00:53, 8.35MB/s] 63%|██████▎   | 735M/1.13G [01:37<00:53, 8.39MB/s] 63%|██████▎   | 735M/1.13G [01:38<00:52, 8.49MB/s] 64%|██████▎   | 736M/1.13G [01:38<00:53, 8.27MB/s] 64%|██████▎   | 737M/1.13G [01:38<00:52, 8.38MB/s] 64%|██████▎   | 738M/1.13G [01:38<01:30, 4.85MB/s] 64%|██████▍   | 740M/1.13G [01:38<01:13, 5.99MB/s] 64%|██████▍   | 741M/1.13G [01:38<01:07, 6.48MB/s] 64%|██████▍   | 741M/1.13G [01:39<01:02, 6.96MB/s] 64%|██████▍   | 742M/1.13G [01:39<01:02, 7.01MB/s] 64%|██████▍   | 743M/1.13G [01:39<00:58, 7.43MB/s] 64%|██████▍   | 744M/1.13G [01:39<00:57, 7.57MB/s] 64%|██████▍   | 745M/1.13G [01:39<00:55, 7.77MB/s] 64%|██████▍   | 745M/1.13G [01:39<00:53, 8.04MB/s] 64%|██████▍   | 746M/1.13G [01:39<00:54, 7.91MB/s] 64%|██████▍   | 747M/1.13G [01:39<00:53, 8.14MB/s] 65%|██████▍   | 748M/1.13G [01:39<00:53, 8.01MB/s] 65%|██████▍   | 749M/1.13G [01:39<00:52, 8.21MB/s] 65%|██████▍   | 749M/1.13G [01:40<00:52, 8.25MB/s] 65%|██████▍   | 750M/1.13G [01:40<00:52, 8.18MB/s] 65%|██████▍   | 751M/1.13G [01:40<00:51, 8.37MB/s] 65%|██████▍   | 752M/1.13G [01:40<00:52, 8.13MB/s] 65%|██████▍   | 753M/1.13G [01:40<00:51, 8.34MB/s] 65%|██████▌   | 754M/1.13G [01:40<00:52, 8.15MB/s] 65%|██████▌   | 754M/1.13G [01:40<00:51, 8.28MB/s] 65%|██████▌   | 755M/1.13G [01:40<00:50, 8.43MB/s] 65%|██████▌   | 756M/1.13G [01:40<00:51, 8.19MB/s] 65%|██████▌   | 757M/1.13G [01:41<00:50, 8.37MB/s] 65%|██████▌   | 758M/1.13G [01:41<00:51, 8.14MB/s] 65%|██████▌   | 759M/1.13G [01:41<00:50, 8.31MB/s] 66%|██████▌   | 759M/1.13G [01:41<00:50, 8.28MB/s] 66%|██████▌   | 760M/1.13G [01:41<00:50, 8.25MB/s] 66%|██████▌   | 761M/1.13G [01:41<00:49, 8.43MB/s] 66%|██████▌   | 762M/1.13G [01:41<00:50, 8.18MB/s] 66%|██████▌   | 763M/1.13G [01:41<00:49, 8.35MB/s] 66%|██████▌   | 763M/1.13G [01:41<00:50, 8.14MB/s] 66%|██████▌   | 764M/1.13G [01:41<00:49, 8.32MB/s] 66%|██████▌   | 765M/1.13G [01:42<00:48, 8.44MB/s] 66%|██████▌   | 766M/1.13G [01:42<00:50, 8.18MB/s] 66%|██████▌   | 767M/1.13G [01:42<00:48, 8.39MB/s] 66%|██████▋   | 768M/1.13G [01:42<00:50, 8.15MB/s] 66%|██████▋   | 768M/1.13G [01:42<00:49, 8.34MB/s] 66%|██████▋   | 769M/1.13G [01:42<00:49, 8.17MB/s] 66%|██████▋   | 770M/1.13G [01:42<00:49, 8.28MB/s] 67%|██████▋   | 771M/1.13G [01:42<01:14, 5.48MB/s] 67%|██████▋   | 772M/1.13G [01:43<01:13, 5.52MB/s] 67%|██████▋   | 773M/1.13G [01:43<01:07, 6.02MB/s] 67%|██████▋   | 774M/1.13G [01:43<01:00, 6.63MB/s] 67%|██████▋   | 774M/1.13G [01:43<00:59, 6.79MB/s] 67%|██████▋   | 775M/1.13G [01:43<00:55, 7.28MB/s] 67%|██████▋   | 776M/1.13G [01:43<00:54, 7.40MB/s] 67%|██████▋   | 777M/1.13G [01:43<00:51, 7.71MB/s] 67%|██████▋   | 778M/1.13G [01:43<00:50, 7.92MB/s] 67%|██████▋   | 778M/1.13G [01:43<00:50, 7.94MB/s] 67%|██████▋   | 779M/1.13G [01:44<00:48, 8.16MB/s] 67%|██████▋   | 780M/1.13G [01:44<00:49, 8.04MB/s] 67%|██████▋   | 781M/1.13G [01:44<00:48, 8.21MB/s] 67%|██████▋   | 782M/1.13G [01:44<00:47, 8.29MB/s] 68%|██████▊   | 782M/1.13G [01:44<00:48, 8.17MB/s] 68%|██████▊   | 783M/1.13G [01:44<00:47, 8.34MB/s] 68%|██████▊   | 784M/1.13G [01:44<00:48, 8.17MB/s] 68%|██████▊   | 785M/1.13G [01:44<00:47, 8.30MB/s] 68%|██████▊   | 786M/1.13G [01:44<00:47, 8.31MB/s] 68%|██████▊   | 786M/1.13G [01:45<00:47, 8.25MB/s] 68%|██████▊   | 787M/1.13G [01:45<00:46, 8.36MB/s] 68%|██████▊   | 788M/1.13G [01:45<00:47, 8.16MB/s] 68%|██████▊   | 789M/1.13G [01:45<00:46, 8.35MB/s] 68%|██████▊   | 790M/1.13G [01:45<00:47, 8.19MB/s] 68%|██████▊   | 791M/1.13G [01:45<00:46, 8.32MB/s] 68%|██████▊   | 791M/1.13G [01:45<00:46, 8.35MB/s] 68%|██████▊   | 792M/1.13G [01:45<00:46, 8.21MB/s] 68%|██████▊   | 793M/1.13G [01:45<00:45, 8.37MB/s] 69%|██████▊   | 794M/1.13G [01:45<00:46, 8.17MB/s] 69%|██████▊   | 795M/1.13G [01:46<00:45, 8.30MB/s] 69%|██████▊   | 795M/1.13G [01:46<00:45, 8.31MB/s] 69%|██████▊   | 796M/1.13G [01:46<00:46, 8.25MB/s] 69%|██████▉   | 797M/1.13G [01:46<00:45, 8.36MB/s] 69%|██████▉   | 798M/1.13G [01:46<00:46, 8.18MB/s] 69%|██████▉   | 799M/1.13G [01:46<00:45, 8.35MB/s] 69%|██████▉   | 800M/1.13G [01:46<00:45, 8.20MB/s] 69%|██████▉   | 800M/1.13G [01:46<00:45, 8.32MB/s] 69%|██████▉   | 801M/1.13G [01:46<00:44, 8.34MB/s] 69%|██████▉   | 802M/1.13G [01:46<00:45, 8.22MB/s] 69%|██████▉   | 803M/1.13G [01:47<00:44, 8.36MB/s] 69%|██████▉   | 804M/1.13G [01:47<00:45, 8.19MB/s] 69%|██████▉   | 804M/1.13G [01:47<00:44, 8.31MB/s] 70%|██████▉   | 805M/1.13G [01:47<00:44, 8.33MB/s] 70%|██████▉   | 806M/1.13G [01:47<00:45, 8.21MB/s] 70%|██████▉   | 807M/1.13G [01:47<00:44, 8.37MB/s] 70%|██████▉   | 808M/1.13G [01:47<00:44, 8.18MB/s] 70%|██████▉   | 809M/1.13G [01:47<00:44, 8.33MB/s] 70%|██████▉   | 809M/1.13G [01:47<00:44, 8.21MB/s] 70%|██████▉   | 810M/1.13G [01:48<00:44, 8.30MB/s] 70%|██████▉   | 811M/1.13G [01:48<00:43, 8.34MB/s] 70%|███████   | 812M/1.13G [01:48<00:44, 8.21MB/s] 70%|███████   | 813M/1.13G [01:48<00:43, 8.37MB/s] 70%|███████   | 813M/1.13G [01:48<00:44, 8.21MB/s] 70%|███████   | 814M/1.13G [01:48<00:43, 8.29MB/s] 70%|███████   | 815M/1.13G [01:48<00:43, 8.33MB/s] 70%|███████   | 816M/1.13G [01:48<00:43, 8.24MB/s] 70%|███████   | 817M/1.13G [01:48<00:43, 8.31MB/s] 71%|███████   | 817M/1.13G [01:48<00:43, 8.20MB/s] 71%|███████   | 818M/1.13G [01:49<00:42, 8.32MB/s] 71%|███████   | 819M/1.13G [01:49<00:43, 8.21MB/s] 71%|███████   | 820M/1.13G [01:49<00:42, 8.27MB/s] 71%|███████   | 821M/1.13G [01:49<00:42, 8.36MB/s] 71%|███████   | 821M/1.13G [01:49<00:43, 8.20MB/s] 71%|███████   | 822M/1.13G [01:49<00:42, 8.35MB/s] 71%|███████   | 823M/1.13G [01:49<00:42, 8.18MB/s] 71%|███████   | 824M/1.13G [01:49<00:42, 8.31MB/s] 71%|███████   | 825M/1.13G [01:49<00:41, 8.39MB/s] 71%|███████▏  | 826M/1.13G [01:49<00:42, 8.24MB/s] 71%|███████▏  | 826M/1.13G [01:50<00:41, 8.31MB/s] 71%|███████▏  | 827M/1.13G [01:50<00:42, 8.22MB/s] 71%|███████▏  | 828M/1.13G [01:50<00:41, 8.30MB/s] 72%|███████▏  | 829M/1.13G [01:50<00:41, 8.24MB/s] 72%|███████▏  | 830M/1.13G [01:50<00:41, 8.22MB/s] 72%|███████▏  | 830M/1.13G [01:50<00:41, 8.38MB/s] 72%|███████▏  | 831M/1.13G [01:50<00:41, 8.22MB/s] 72%|███████▏  | 832M/1.13G [01:50<00:40, 8.37MB/s] 72%|███████▏  | 833M/1.13G [01:50<00:41, 8.19MB/s] 72%|███████▏  | 834M/1.13G [01:50<00:40, 8.32MB/s] 72%|███████▏  | 835M/1.13G [01:51<00:40, 8.35MB/s] 72%|███████▏  | 835M/1.13G [01:51<01:11, 4.74MB/s] 72%|███████▏  | 837M/1.13G [01:51<00:55, 6.02MB/s] 72%|███████▏  | 838M/1.13G [01:51<00:51, 6.50MB/s] 72%|███████▏  | 839M/1.13G [01:51<00:48, 6.95MB/s] 72%|███████▏  | 840M/1.13G [01:52<00:47, 7.10MB/s] 73%|███████▎  | 841M/1.13G [01:52<00:44, 7.49MB/s] 73%|███████▎  | 841M/1.13G [01:52<00:44, 7.55MB/s] 73%|███████▎  | 842M/1.13G [01:52<00:42, 7.86MB/s] 73%|███████▎  | 843M/1.13G [01:52<00:40, 8.08MB/s] 73%|███████▎  | 844M/1.13G [01:52<00:41, 7.97MB/s] 73%|███████▎  | 845M/1.13G [01:52<00:40, 8.18MB/s] 73%|███████▎  | 845M/1.13G [01:52<00:40, 8.04MB/s] 73%|███████▎  | 846M/1.13G [01:52<00:39, 8.21MB/s] 73%|███████▎  | 847M/1.13G [01:52<00:39, 8.18MB/s] 73%|███████▎  | 848M/1.13G [01:53<00:44, 7.33MB/s] 73%|███████▎  | 849M/1.13G [01:53<00:38, 8.52MB/s] 73%|███████▎  | 850M/1.13G [01:53<00:38, 8.43MB/s] 73%|███████▎  | 851M/1.13G [01:53<00:37, 8.52MB/s] 73%|███████▎  | 851M/1.13G [01:53<00:38, 8.28MB/s] 74%|███████▎  | 852M/1.13G [01:53<00:38, 8.40MB/s] 74%|███████▎  | 853M/1.13G [01:53<00:38, 8.23MB/s] 74%|███████▎  | 854M/1.13G [01:53<00:38, 8.33MB/s] 74%|███████▍  | 855M/1.13G [01:53<00:37, 8.45MB/s] 74%|███████▍  | 856M/1.13G [01:54<00:38, 8.23MB/s] 74%|███████▍  | 856M/1.13G [01:54<00:37, 8.34MB/s] 74%|███████▍  | 857M/1.13G [01:54<00:38, 8.18MB/s] 74%|███████▍  | 858M/1.13G [01:54<00:37, 8.34MB/s] 74%|███████▍  | 859M/1.13G [01:54<00:38, 8.18MB/s] 74%|███████▍  | 860M/1.13G [01:54<00:37, 8.31MB/s] 74%|███████▍  | 861M/1.13G [01:54<00:37, 8.42MB/s] 74%|███████▍  | 861M/1.13G [01:54<00:37, 8.21MB/s] 74%|███████▍  | 862M/1.13G [01:54<00:37, 8.35MB/s] 74%|███████▍  | 863M/1.13G [01:54<00:37, 8.20MB/s] 75%|███████▍  | 864M/1.13G [01:55<00:37, 8.33MB/s] 75%|███████▍  | 865M/1.13G [01:55<00:36, 8.36MB/s] 75%|███████▍  | 865M/1.13G [01:55<00:37, 8.23MB/s] 75%|███████▍  | 866M/1.13G [01:55<00:36, 8.35MB/s] 75%|███████▍  | 867M/1.13G [01:55<00:37, 8.17MB/s] 75%|███████▍  | 868M/1.13G [01:55<00:36, 8.30MB/s] 75%|███████▍  | 869M/1.13G [01:55<00:37, 8.16MB/s] 75%|███████▌  | 870M/1.13G [01:55<00:36, 8.30MB/s] 75%|███████▌  | 870M/1.13G [01:55<00:35, 8.41MB/s] 75%|███████▌  | 871M/1.13G [01:55<00:36, 8.22MB/s] 75%|███████▌  | 872M/1.13G [01:56<00:36, 8.30MB/s] 75%|███████▌  | 873M/1.13G [01:56<00:36, 8.20MB/s] 75%|███████▌  | 874M/1.13G [01:56<00:35, 8.31MB/s] 75%|███████▌  | 874M/1.13G [01:56<01:27, 3.42MB/s] 76%|███████▌  | 875M/1.13G [01:56<01:10, 4.22MB/s] 76%|███████▌  | 876M/1.13G [01:57<00:59, 5.02MB/s] 76%|███████▌  | 877M/1.13G [01:57<00:53, 5.51MB/s] 76%|███████▌  | 878M/1.13G [01:57<00:47, 6.24MB/s] 76%|███████▌  | 878M/1.13G [01:57<00:44, 6.57MB/s] 76%|███████▌  | 879M/1.13G [01:57<00:41, 7.13MB/s] 76%|███████▌  | 880M/1.13G [01:57<00:39, 7.33MB/s] 76%|███████▌  | 881M/1.13G [01:57<00:37, 7.67MB/s] 76%|███████▌  | 882M/1.13G [01:57<00:36, 7.96MB/s] 76%|███████▌  | 883M/1.13G [01:57<00:36, 7.88MB/s] 76%|███████▌  | 883M/1.13G [01:58<00:35, 8.12MB/s] 76%|███████▋  | 884M/1.13G [01:58<00:35, 8.01MB/s] 76%|███████▋  | 885M/1.13G [01:58<00:34, 8.20MB/s] 76%|███████▋  | 886M/1.13G [01:58<00:34, 8.39MB/s] 77%|███████▋  | 887M/1.13G [01:58<00:34, 8.15MB/s] 77%|███████▋  | 887M/1.13G [01:58<00:34, 8.31MB/s] 77%|███████▋  | 888M/1.13G [01:58<00:34, 8.11MB/s] 77%|███████▋  | 889M/1.13G [01:58<00:34, 8.29MB/s] 77%|███████▋  | 890M/1.13G [01:58<00:34, 8.14MB/s] 77%|███████▋  | 891M/1.13G [01:58<00:33, 8.31MB/s] 77%|███████▋  | 892M/1.13G [01:59<00:33, 8.44MB/s] 77%|███████▋  | 892M/1.13G [01:59<00:34, 8.20MB/s] 77%|███████▋  | 893M/1.13G [01:59<00:33, 8.35MB/s] 77%|███████▋  | 894M/1.13G [01:59<00:34, 8.14MB/s] 77%|███████▋  | 895M/1.13G [01:59<00:33, 8.31MB/s] 77%|███████▋  | 896M/1.13G [01:59<00:33, 8.26MB/s] 77%|███████▋  | 897M/1.13G [01:59<00:33, 8.24MB/s] 77%|███████▋  | 897M/1.13G [01:59<00:32, 8.42MB/s] 78%|███████▊  | 898M/1.13G [01:59<00:33, 8.18MB/s] 78%|███████▊  | 899M/1.13G [01:59<00:32, 8.34MB/s] 78%|███████▊  | 900M/1.13G [02:00<00:33, 8.14MB/s] 78%|███████▊  | 901M/1.13G [02:00<00:32, 8.30MB/s] 78%|███████▊  | 902M/1.13G [02:00<00:31, 8.47MB/s] 78%|███████▊  | 902M/1.13G [02:00<00:32, 8.21MB/s] 78%|███████▊  | 903M/1.13G [02:00<00:31, 8.38MB/s] 78%|███████▊  | 904M/1.13G [02:00<00:32, 8.16MB/s] 78%|███████▊  | 905M/1.13G [02:00<00:31, 8.32MB/s] 78%|███████▊  | 906M/1.13G [02:00<00:32, 8.15MB/s] 78%|███████▊  | 906M/1.13G [02:00<00:31, 8.29MB/s] 78%|███████▊  | 907M/1.13G [02:01<00:31, 8.43MB/s] 78%|███████▊  | 908M/1.13G [02:01<00:32, 8.20MB/s] 78%|███████▊  | 909M/1.13G [02:01<00:31, 8.34MB/s] 79%|███████▊  | 910M/1.13G [02:01<00:32, 8.15MB/s] 79%|███████▊  | 911M/1.13G [02:01<00:31, 8.35MB/s] 79%|███████▊  | 911M/1.13G [02:01<00:31, 8.20MB/s] 79%|███████▊  | 912M/1.13G [02:01<00:31, 8.27MB/s] 79%|███████▉  | 913M/1.13G [02:01<00:30, 8.40MB/s] 79%|███████▉  | 914M/1.13G [02:01<00:31, 8.18MB/s] 79%|███████▉  | 915M/1.13G [02:01<00:30, 8.34MB/s] 79%|███████▉  | 915M/1.13G [02:02<00:31, 8.17MB/s] 79%|███████▉  | 916M/1.13G [02:03<01:50, 2.31MB/s] 80%|███████▉  | 922M/1.13G [02:03<00:29, 8.48MB/s] 80%|███████▉  | 924M/1.13G [02:03<00:29, 8.44MB/s] 80%|███████▉  | 926M/1.13G [02:03<00:29, 8.38MB/s] 80%|████████  | 927M/1.13G [02:03<00:29, 8.32MB/s] 80%|████████  | 929M/1.13G [02:04<00:29, 8.28MB/s] 80%|████████  | 930M/1.13G [02:04<00:28, 8.33MB/s] 80%|████████  | 931M/1.13G [02:04<00:28, 8.25MB/s] 80%|████████  | 932M/1.13G [02:04<00:28, 8.34MB/s] 81%|████████  | 933M/1.13G [02:04<00:28, 8.23MB/s] 81%|████████  | 934M/1.13G [02:04<00:37, 6.33MB/s] 81%|████████  | 935M/1.13G [02:04<00:26, 8.97MB/s] 81%|████████  | 937M/1.13G [02:04<00:26, 8.79MB/s] 81%|████████  | 938M/1.13G [02:05<00:26, 8.68MB/s] 81%|████████  | 938M/1.13G [02:05<00:26, 8.69MB/s] 81%|████████  | 939M/1.13G [02:05<00:27, 8.46MB/s] 81%|████████  | 940M/1.13G [02:05<00:26, 8.51MB/s] 81%|████████  | 941M/1.13G [02:05<00:27, 8.27MB/s] 81%|████████▏ | 942M/1.13G [02:05<00:26, 8.41MB/s] 81%|████████▏ | 943M/1.13G [02:05<00:27, 8.24MB/s] 81%|████████▏ | 944M/1.13G [02:05<00:27, 8.34MB/s] 82%|████████▏ | 944M/1.13G [02:05<00:27, 8.24MB/s] 82%|████████▏ | 945M/1.13G [02:06<00:27, 8.03MB/s] 82%|████████▏ | 946M/1.13G [02:06<00:26, 8.52MB/s] 82%|████████▏ | 947M/1.13G [02:06<00:26, 8.26MB/s] 82%|████████▏ | 948M/1.13G [02:06<00:26, 8.42MB/s] 82%|████████▏ | 949M/1.13G [02:06<00:26, 8.20MB/s] 82%|████████▏ | 949M/1.13G [02:06<00:26, 8.31MB/s] 82%|████████▏ | 950M/1.13G [02:06<00:26, 8.35MB/s] 82%|████████▏ | 951M/1.13G [02:06<00:26, 8.25MB/s] 82%|████████▏ | 952M/1.13G [02:06<00:25, 8.39MB/s] 82%|████████▏ | 953M/1.13G [02:07<00:26, 8.16MB/s] 82%|████████▏ | 954M/1.13G [02:07<00:25, 8.33MB/s] 82%|████████▏ | 954M/1.13G [02:07<01:18, 2.72MB/s] 83%|████████▎ | 961M/1.13G [02:08<00:20, 10.3MB/s] 83%|████████▎ | 963M/1.13G [02:08<00:21, 9.67MB/s] 83%|████████▎ | 965M/1.13G [02:08<00:21, 9.29MB/s] 83%|████████▎ | 966M/1.13G [02:08<00:22, 9.00MB/s] 84%|████████▎ | 968M/1.13G [02:08<00:22, 8.95MB/s] 84%|████████▎ | 969M/1.13G [02:09<00:22, 8.72MB/s] 84%|████████▎ | 970M/1.13G [02:09<00:23, 8.60MB/s] 84%|████████▍ | 971M/1.13G [02:09<00:23, 8.53MB/s] 84%|████████▍ | 972M/1.13G [02:09<00:22, 8.62MB/s] 84%|████████▍ | 973M/1.13G [02:09<00:23, 8.40MB/s] 84%|████████▍ | 973M/1.13G [02:09<00:22, 8.47MB/s] 84%|████████▍ | 974M/1.13G [02:09<00:23, 8.25MB/s] 84%|████████▍ | 975M/1.13G [02:09<00:22, 8.41MB/s] 84%|████████▍ | 976M/1.13G [02:09<00:23, 8.18MB/s] 84%|████████▍ | 977M/1.13G [02:10<00:22, 8.34MB/s] 84%|████████▍ | 978M/1.13G [02:10<00:22, 8.29MB/s] 84%|████████▍ | 978M/1.13G [02:10<00:22, 8.27MB/s] 85%|████████▍ | 979M/1.13G [02:10<00:22, 8.40MB/s] 85%|████████▍ | 980M/1.13G [02:10<00:22, 8.17MB/s] 85%|████████▍ | 981M/1.13G [02:10<00:22, 8.34MB/s] 85%|████████▍ | 982M/1.13G [02:10<00:22, 8.17MB/s] 85%|████████▍ | 983M/1.13G [02:10<00:22, 8.24MB/s] 85%|████████▍ | 983M/1.13G [02:10<00:21, 8.46MB/s] 85%|████████▍ | 984M/1.13G [02:11<00:22, 8.23MB/s] 85%|████████▌ | 985M/1.13G [02:11<00:21, 8.39MB/s] 85%|████████▌ | 986M/1.13G [02:11<00:22, 8.21MB/s] 85%|████████▌ | 987M/1.13G [02:11<00:21, 8.34MB/s] 85%|████████▌ | 988M/1.13G [02:11<00:21, 8.18MB/s] 85%|████████▌ | 988M/1.13G [02:11<00:21, 8.29MB/s] 85%|████████▌ | 989M/1.13G [02:11<00:21, 8.42MB/s] 85%|████████▌ | 990M/1.13G [02:11<00:21, 8.16MB/s] 86%|████████▌ | 991M/1.13G [02:11<00:20, 8.38MB/s] 86%|████████▌ | 992M/1.13G [02:11<00:21, 8.13MB/s] 86%|████████▌ | 992M/1.13G [02:12<00:20, 8.31MB/s] 86%|████████▌ | 993M/1.13G [02:12<00:20, 8.31MB/s] 86%|████████▌ | 994M/1.13G [02:12<00:20, 8.23MB/s] 86%|████████▌ | 995M/1.13G [02:12<00:20, 8.40MB/s] 86%|████████▌ | 996M/1.13G [02:12<00:20, 8.14MB/s] 86%|████████▌ | 997M/1.13G [02:12<00:20, 8.31MB/s] 86%|████████▌ | 997M/1.13G [02:12<00:20, 8.15MB/s] 86%|████████▌ | 998M/1.13G [02:12<00:20, 8.34MB/s] 86%|████████▌ | 999M/1.13G [02:12<00:19, 8.47MB/s] 86%|████████▋ | 0.98G/1.13G [02:13<00:20, 8.19MB/s] 86%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.39MB/s] 86%|████████▋ | 0.98G/1.13G [02:13<00:20, 8.18MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.35MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.20MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.27MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.44MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.17MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.34MB/s] 87%|████████▋ | 0.98G/1.13G [02:13<00:19, 8.13MB/s] 87%|████████▋ | 0.98G/1.13G [02:14<00:19, 8.25MB/s] 87%|████████▋ | 0.99G/1.13G [02:14<00:18, 8.35MB/s] 87%|████████▋ | 0.99G/1.13G [02:14<00:18, 8.26MB/s] 87%|████████▋ | 0.99G/1.13G [02:14<00:18, 8.40MB/s] 87%|████████▋ | 0.99G/1.13G [02:14<00:18, 8.18MB/s] 87%|████████▋ | 0.99G/1.13G [02:14<00:18, 8.33MB/s] 87%|████████▋ | 0.99G/1.13G [02:14<00:18, 8.14MB/s] 88%|████████▊ | 0.99G/1.13G [02:14<00:18, 8.30MB/s] 88%|████████▊ | 0.99G/1.13G [02:14<00:17, 8.46MB/s] 88%|████████▊ | 0.99G/1.13G [02:14<00:18, 8.20MB/s] 88%|████████▊ | 0.99G/1.13G [02:15<00:17, 8.35MB/s] 88%|████████▊ | 0.99G/1.13G [02:15<00:18, 8.14MB/s] 88%|████████▊ | 0.99G/1.13G [02:15<00:17, 8.27MB/s] 88%|████████▊ | 0.99G/1.13G [02:15<00:17, 8.31MB/s] 88%|████████▊ | 1.00G/1.13G [02:15<00:17, 8.23MB/s] 88%|████████▊ | 1.00G/1.13G [02:15<00:17, 8.38MB/s] 88%|████████▊ | 1.00G/1.13G [02:15<00:17, 8.16MB/s] 88%|████████▊ | 1.00G/1.13G [02:16<00:27, 5.17MB/s] 88%|████████▊ | 1.00G/1.13G [02:16<00:24, 5.69MB/s] 88%|████████▊ | 1.00G/1.13G [02:16<00:22, 6.26MB/s] 88%|████████▊ | 1.00G/1.13G [02:16<00:20, 6.78MB/s] 89%|████████▊ | 1.00G/1.13G [02:16<00:19, 6.98MB/s] 89%|████████▊ | 1.00G/1.13G [02:17<00:42, 3.26MB/s] 89%|████████▊ | 1.00G/1.13G [02:17<00:34, 4.02MB/s] 89%|████████▉ | 1.00G/1.13G [02:17<00:28, 4.81MB/s] 89%|████████▉ | 1.00G/1.13G [02:17<00:25, 5.33MB/s] 89%|████████▉ | 1.01G/1.13G [02:17<00:22, 6.08MB/s] 89%|████████▉ | 1.01G/1.13G [02:17<00:20, 6.45MB/s] 89%|████████▉ | 1.01G/1.13G [02:17<00:30, 4.43MB/s] 89%|████████▉ | 1.01G/1.13G [02:18<00:24, 5.39MB/s] 89%|████████▉ | 1.01G/1.13G [02:18<00:22, 5.92MB/s] 89%|████████▉ | 1.01G/1.13G [02:18<00:19, 6.58MB/s] 89%|████████▉ | 1.01G/1.13G [02:18<00:19, 6.76MB/s] 89%|████████▉ | 1.01G/1.13G [02:18<00:17, 7.14MB/s] 90%|████████▉ | 1.01G/1.13G [02:18<00:17, 7.40MB/s] 90%|████████▉ | 1.01G/1.13G [02:18<00:16, 7.67MB/s] 90%|████████▉ | 1.01G/1.13G [02:18<00:15, 8.14MB/s] 90%|████████▉ | 1.02G/1.13G [02:19<00:15, 7.88MB/s] 90%|████████▉ | 1.02G/1.13G [02:19<00:15, 8.12MB/s] 90%|████████▉ | 1.02G/1.13G [02:19<00:15, 8.03MB/s] 90%|████████▉ | 1.02G/1.13G [02:19<00:15, 8.12MB/s] 90%|█████████ | 1.02G/1.13G [02:19<00:14, 8.21MB/s] 90%|█████████ | 1.02G/1.13G [02:19<00:14, 8.16MB/s] 90%|█████████ | 1.02G/1.13G [02:19<00:14, 8.45MB/s] 90%|█████████ | 1.02G/1.13G [02:19<00:14, 8.15MB/s] 90%|█████████ | 1.02G/1.13G [02:19<00:14, 8.23MB/s] 90%|█████████ | 1.02G/1.13G [02:19<00:14, 8.21MB/s] 90%|█████████ | 1.02G/1.13G [02:20<00:14, 8.24MB/s] 91%|█████████ | 1.02G/1.13G [02:20<00:13, 8.63MB/s] 91%|█████████ | 1.02G/1.13G [02:20<00:14, 8.18MB/s] 91%|█████████ | 1.03G/1.13G [02:20<00:13, 8.35MB/s] 91%|█████████ | 1.03G/1.13G [02:20<00:13, 8.16MB/s] 91%|█████████ | 1.03G/1.13G [02:20<00:13, 8.18MB/s] 91%|█████████ | 1.03G/1.13G [02:20<00:13, 8.22MB/s] 91%|█████████ | 1.03G/1.13G [02:20<00:13, 8.25MB/s] 91%|█████████ | 1.03G/1.13G [02:20<00:12, 8.51MB/s] 91%|█████████ | 1.03G/1.13G [02:21<00:13, 8.16MB/s] 91%|█████████ | 1.03G/1.13G [02:21<00:12, 8.30MB/s] 91%|█████████ | 1.03G/1.13G [02:21<00:13, 8.21MB/s] 91%|█████████▏| 1.03G/1.13G [02:21<00:12, 8.26MB/s] 91%|█████████▏| 1.03G/1.13G [02:21<00:12, 8.31MB/s] 91%|█████████▏| 1.03G/1.13G [02:21<00:12, 8.24MB/s] 91%|█████████▏| 1.04G/1.13G [02:21<00:12, 8.36MB/s] 92%|█████████▏| 1.04G/1.13G [02:21<00:12, 8.25MB/s] 92%|█████████▏| 1.04G/1.13G [02:21<00:12, 8.21MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:18, 5.47MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:18, 5.45MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:20, 4.76MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:13, 7.29MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:12, 7.60MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:12, 7.66MB/s] 92%|█████████▏| 1.04G/1.13G [02:22<00:11, 7.90MB/s] 92%|█████████▏| 1.04G/1.13G [02:23<00:11, 8.10MB/s] 92%|█████████▏| 1.05G/1.13G [02:23<00:11, 8.00MB/s] 92%|█████████▏| 1.05G/1.13G [02:23<00:11, 8.15MB/s] 93%|█████████▎| 1.05G/1.13G [02:23<00:11, 8.08MB/s] 93%|█████████▎| 1.05G/1.13G [02:23<00:10, 8.23MB/s] 93%|█████████▎| 1.05G/1.13G [02:23<00:10, 8.12MB/s] 93%|█████████▎| 1.05G/1.13G [02:23<00:10, 8.25MB/s] 93%|█████████▎| 1.05G/1.13G [02:23<00:10, 8.35MB/s] 93%|█████████▎| 1.05G/1.13G [02:23<00:10, 8.17MB/s] 93%|█████████▎| 1.05G/1.13G [02:24<00:17, 4.78MB/s] 93%|█████████▎| 1.05G/1.13G [02:24<00:13, 6.04MB/s] 93%|█████████▎| 1.05G/1.13G [02:24<00:12, 6.53MB/s] 93%|█████████▎| 1.05G/1.13G [02:24<00:11, 6.99MB/s] 93%|█████████▎| 1.06G/1.13G [02:24<00:11, 7.12MB/s] 93%|█████████▎| 1.06G/1.13G [02:24<00:10, 7.51MB/s] 93%|█████████▎| 1.06G/1.13G [02:25<00:10, 7.55MB/s] 94%|█████████▎| 1.06G/1.13G [02:25<00:17, 4.52MB/s] 94%|█████████▎| 1.06G/1.13G [02:25<00:08, 8.79MB/s] 94%|█████████▍| 1.06G/1.13G [02:25<00:08, 9.21MB/s] 94%|█████████▍| 1.06G/1.13G [02:25<00:08, 8.84MB/s] 94%|█████████▍| 1.06G/1.13G [02:25<00:08, 8.82MB/s] 94%|█████████▍| 1.06G/1.13G [02:25<00:08, 8.56MB/s] 94%|█████████▍| 1.07G/1.13G [02:26<00:08, 8.60MB/s] 94%|█████████▍| 1.07G/1.13G [02:26<00:08, 8.61MB/s] 94%|█████████▍| 1.07G/1.13G [02:26<00:08, 8.37MB/s] 94%|█████████▍| 1.07G/1.13G [02:26<00:07, 8.48MB/s] 95%|█████████▍| 1.07G/1.13G [02:26<00:08, 8.27MB/s] 95%|█████████▍| 1.07G/1.13G [02:26<00:07, 8.35MB/s] 95%|█████████▍| 1.07G/1.13G [02:26<00:07, 8.21MB/s] 95%|█████████▍| 1.07G/1.13G [02:27<00:13, 4.87MB/s] 95%|█████████▍| 1.07G/1.13G [02:27<00:10, 6.07MB/s] 95%|█████████▍| 1.07G/1.13G [02:27<00:19, 3.18MB/s] 95%|█████████▌| 1.08G/1.13G [02:28<00:08, 6.71MB/s] 95%|█████████▌| 1.08G/1.13G [02:28<00:08, 7.04MB/s] 95%|█████████▌| 1.08G/1.13G [02:28<00:07, 7.31MB/s] 96%|█████████▌| 1.08G/1.13G [02:28<00:07, 7.51MB/s] 96%|█████████▌| 1.08G/1.13G [02:28<00:06, 7.69MB/s] 96%|█████████▌| 1.08G/1.13G [02:28<00:06, 7.81MB/s] 96%|█████████▌| 1.08G/1.13G [02:28<00:06, 7.93MB/s] 96%|█████████▌| 1.08G/1.13G [02:29<00:06, 8.02MB/s] 96%|█████████▌| 1.09G/1.13G [02:29<00:06, 8.05MB/s] 96%|█████████▌| 1.09G/1.13G [02:29<00:06, 8.11MB/s] 96%|█████████▌| 1.09G/1.13G [02:29<00:05, 8.17MB/s] 96%|█████████▌| 1.09G/1.13G [02:29<00:05, 8.24MB/s] 96%|█████████▌| 1.09G/1.13G [02:29<00:05, 8.22MB/s] 96%|█████████▋| 1.09G/1.13G [02:29<00:05, 8.24MB/s] 96%|█████████▋| 1.09G/1.13G [02:29<00:05, 8.25MB/s] 96%|█████████▋| 1.09G/1.13G [02:29<00:05, 8.26MB/s] 96%|█████████▋| 1.09G/1.13G [02:29<00:05, 8.30MB/s] 97%|█████████▋| 1.09G/1.13G [02:30<00:05, 8.26MB/s] 97%|█████████▋| 1.09G/1.13G [02:30<00:04, 8.28MB/s] 97%|█████████▋| 1.09G/1.13G [02:30<00:04, 8.25MB/s] 97%|█████████▋| 1.09G/1.13G [02:30<00:04, 8.21MB/s] 97%|█████████▋| 1.10G/1.13G [02:30<00:04, 8.27MB/s] 97%|█████████▋| 1.10G/1.13G [02:30<00:04, 8.30MB/s] 97%|█████████▋| 1.10G/1.13G [02:30<00:04, 8.31MB/s] 97%|█████████▋| 1.10G/1.13G [02:30<00:04, 8.27MB/s] 97%|█████████▋| 1.10G/1.13G [02:30<00:04, 8.19MB/s] 97%|█████████▋| 1.10G/1.13G [02:30<00:04, 8.33MB/s] 97%|█████████▋| 1.10G/1.13G [02:31<00:04, 8.31MB/s] 97%|█████████▋| 1.10G/1.13G [02:31<00:03, 8.28MB/s] 97%|█████████▋| 1.10G/1.13G [02:31<00:03, 8.28MB/s] 97%|█████████▋| 1.10G/1.13G [02:31<00:03, 8.26MB/s] 98%|█████████▊| 1.10G/1.13G [02:31<00:03, 8.30MB/s] 98%|█████████▊| 1.10G/1.13G [02:31<00:03, 8.25MB/s] 98%|█████████▊| 1.10G/1.13G [02:31<00:03, 8.30MB/s] 98%|█████████▊| 1.11G/1.13G [02:31<00:03, 8.29MB/s] 98%|█████████▊| 1.11G/1.13G [02:31<00:03, 8.28MB/s] 98%|█████████▊| 1.11G/1.13G [02:31<00:03, 8.29MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:03, 8.29MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.31MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.28MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.29MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.26MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.24MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.29MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.27MB/s] 98%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.28MB/s] 99%|█████████▊| 1.11G/1.13G [02:32<00:02, 8.31MB/s] 99%|█████████▊| 1.12G/1.13G [02:33<00:02, 8.21MB/s] 99%|█████████▊| 1.12G/1.13G [02:33<00:01, 8.31MB/s] 99%|█████████▊| 1.12G/1.13G [02:33<00:01, 8.24MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.31MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.26MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.24MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.33MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.27MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.30MB/s] 99%|█████████▉| 1.12G/1.13G [02:33<00:01, 8.30MB/s] 99%|█████████▉| 1.12G/1.13G [02:34<00:01, 8.27MB/s] 99%|█████████▉| 1.12G/1.13G [02:34<00:00, 8.28MB/s] 99%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.30MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.35MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.27MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.27MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.23MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.24MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.29MB/s]100%|█████████▉| 1.13G/1.13G [02:34<00:00, 8.27MB/s]100%|█████████▉| 1.13G/1.13G [02:35<00:00, 8.28MB/s]100%|██████████| 1.13G/1.13G [02:35<00:00, 7.83MB/s]
2022-09-21 17:25:19,776 - mmseg - INFO - Resize the pos_embed shape from torch.Size([1, 577, 1024]) to torch.Size([1, 1601, 1024])
train.py:207: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  'SyncBN is only supported with DDP. To be compatible with DP, '
2022-09-21 17:25:21,250 - mmseg - INFO - EncoderDecoder(
  (backbone): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (12): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (13): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (14): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (15): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (16): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (17): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (18): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (19): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (20): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (21): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (22): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (23): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth'}
  (decode_head): SegmenterMaskTransformerHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (dec_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (patch_proj): Linear(in_features=1024, out_features=1024, bias=False)
    (classes_proj): Linear(in_features=1024, out_features=1024, bias=False)
    (decoder_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (mask_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2022-09-21 17:25:21,348 - mmseg - INFO - Loaded 2296 images
2022-09-21 17:25:21,431 - mmseg - INFO - Loaded 764 images
2022-09-21 17:25:25,301 - mmseg - INFO - Loaded 764 images
2022-09-21 17:25:25,306 - mmseg - INFO - Start running, host: kcarvajal@nukwa-05.cnca, work_dir: /work/kcarvajal/mmsegmentation_Nematodos/work_dirs/segmenterL
2022-09-21 17:25:25,307 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) MMSegWandbHook                     
 -------------------- 
2022-09-21 17:25:25,307 - mmseg - INFO - workflow: [('train', 1), ('val', 1)], max: 160000 iters
2022-09-21 17:25:25,307 - mmseg - INFO - Checkpoints will be saved to /work/kcarvajal/mmsegmentation_Nematodos/work_dirs/segmenterL by HardDiskBackend.
Traceback (most recent call last):
  File "train.py", line 242, in <module>
    main()
  File "train.py", line 238, in main
    meta=meta)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/apis/train.py", line 194, in train_segmentor
    runner.run(data_loaders, cfg.workflow)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/iter_based_runner.py", line 126, in run
    self.call_hook('before_run')
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_runner.py", line 317, in call_hook
    getattr(hook, fn_name)(self)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/dist_utils.py", line 135, in wrapper
    return func(*args, **kwargs)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/core/hook/wandblogger_hook.py", line 106, in before_run
    super(MMSegWandbHook, self).before_run(runner)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/dist_utils.py", line 135, in wrapper
    return func(*args, **kwargs)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/hooks/logger/wandb.py", line 83, in before_run
    self.wandb.init(**self.init_kwargs)  # type: ignore
AttributeError: module 'wandb' has no attribute 'init'
2022-09-21 17:25:39,588 - mmseg - INFO - Multi-processing start method is `None`
2022-09-21 17:25:39,591 - mmseg - INFO - OpenCV num_threads is `48
2022-09-21 17:25:39,807 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.27.0+45dfd3f
------------------------------------------------------------

2022-09-21 17:25:39,807 - mmseg - INFO - Distributed training: False
2022-09-21 17:25:41,593 - mmseg - INFO - Config:
checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth'
backbone_norm_cfg = dict(type='LN', eps=1e-06, requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=
    'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=(640, 640),
        patch_size=16,
        in_channels=3,
        embed_dims=1024,
        num_layers=24,
        num_heads=16,
        drop_path_rate=0.1,
        attn_drop_rate=0.0,
        drop_rate=0.0,
        final_norm=True,
        norm_cfg=dict(type='LN', eps=1e-06, requires_grad=True),
        with_cls_token=True,
        interpolate_mode='bicubic'),
    decode_head=dict(
        type='SegmenterMaskTransformerHead',
        in_channels=1024,
        channels=1024,
        num_classes=2,
        num_layers=2,
        num_heads=16,
        embed_dims=1024,
        dropout_ratio=0.0,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    test_cfg=dict(mode='slide', crop_size=(640, 640), stride=(608, 608)))
dataset_type = 'NematodosDataset'
data_root = '../data/nematodos'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (768, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
    dict(type='RandomRotate', prob=0.75, degree=30),
    dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 768),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=2,
    train=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/test.txt'),
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=2, shuffle=False))
log_config = dict(
    interval=16000,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False, interval=8000),
        dict(
            type='MMSegWandbHook',
            with_step=False,
            init_kwargs=dict(
                entity='seg_nematodos',
                project='Nematodos',
                name='segmenterL_pretrain',
                id='segmenterL_pretrain',
                resume='allow',
                notes=
                'Entrenamiento modelo segmenterL pretrain, batch=1, lr=0.001, m=0.9, 160k iter'
            ),
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'https://download.openmmlab.com/mmsegmentation/v0.5/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth'
resume_from = None
workflow = [('train', 1), ('val', 1)]
cudnn_benchmark = True
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0)
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000, max_keep_ckpts=2)
evaluation = dict(
    interval=16000, metric=['mIoU', 'mDice', 'mFscore'], pre_eval=True)
work_dir = '../work_dirs/segmenterL_pretrain'
seed = 0
gpu_ids = [0]
device = 'cuda'
auto_resume = False

2022-09-21 17:25:41,593 - mmseg - INFO - Set random seed to 1994764831, deterministic: False
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
2022-09-21 17:25:43,931 - mmseg - INFO - load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth
2022-09-21 17:25:44,501 - mmseg - INFO - Resize the pos_embed shape from torch.Size([1, 577, 1024]) to torch.Size([1, 1601, 1024])
train.py:207: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  'SyncBN is only supported with DDP. To be compatible with DP, '
2022-09-21 17:25:45,737 - mmseg - INFO - EncoderDecoder(
  (backbone): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (12): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (13): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (14): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (15): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (16): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (17): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (18): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (19): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (20): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (21): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (22): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (23): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth'}
  (decode_head): SegmenterMaskTransformerHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (dec_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (patch_proj): Linear(in_features=1024, out_features=1024, bias=False)
    (classes_proj): Linear(in_features=1024, out_features=1024, bias=False)
    (decoder_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (mask_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2022-09-21 17:25:45,747 - mmseg - INFO - Loaded 2296 images
2022-09-21 17:25:45,749 - mmseg - INFO - Loaded 764 images
2022-09-21 17:25:49,881 - mmseg - INFO - Loaded 764 images
2022-09-21 17:25:49,882 - mmseg - INFO - load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth
Downloading: "https://download.openmmlab.com/mmsegmentation/v0.5/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth" to /home/kcarvajal/.cache/torch/hub/checkpoints/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth
  0%|          | 0.00/1.24G [00:00<?, ?B/s]  0%|          | 80.0k/1.24G [00:00<28:23, 782kB/s]  0%|          | 160k/1.24G [00:00<28:22, 783kB/s]   0%|          | 312k/1.24G [00:00<19:55, 1.11MB/s]  0%|          | 616k/1.24G [00:00<11:50, 1.87MB/s]  0%|          | 1.16M/1.24G [00:00<06:43, 3.30MB/s]  0%|          | 2.24M/1.24G [00:00<03:42, 5.97MB/s]  0%|          | 3.55M/1.24G [00:00<02:37, 8.45MB/s]  0%|          | 4.36M/1.24G [00:00<02:40, 8.30MB/s]  0%|          | 5.16M/1.24G [00:00<02:47, 7.94MB/s]  0%|          | 6.05M/1.24G [00:01<02:39, 8.33MB/s]  1%|          | 6.94M/1.24G [00:01<02:33, 8.62MB/s]  1%|          | 7.81M/1.24G [00:01<02:31, 8.77MB/s]  1%|          | 8.73M/1.24G [00:01<02:26, 9.02MB/s]  1%|          | 9.60M/1.24G [00:01<02:27, 8.99MB/s]  1%|          | 10.5M/1.24G [00:01<02:25, 9.10MB/s]  1%|          | 11.4M/1.24G [00:01<02:24, 9.12MB/s]  1%|          | 12.3M/1.24G [00:01<02:23, 9.18MB/s]  1%|          | 13.1M/1.24G [00:01<02:25, 9.09MB/s]  1%|          | 14.1M/1.24G [00:01<02:22, 9.24MB/s]  1%|          | 14.9M/1.24G [00:02<02:23, 9.21MB/s]  1%|          | 15.8M/1.24G [00:02<02:22, 9.24MB/s]  1%|▏         | 16.7M/1.24G [00:02<02:21, 9.28MB/s]  1%|▏         | 17.6M/1.24G [00:02<02:22, 9.25MB/s]  1%|▏         | 18.5M/1.24G [00:02<02:21, 9.29MB/s]  2%|▏         | 19.4M/1.24G [00:02<02:23, 9.15MB/s]  2%|▏         | 20.3M/1.24G [00:02<02:21, 9.29MB/s]  2%|▏         | 21.2M/1.24G [00:02<02:22, 9.18MB/s]  2%|▏         | 22.1M/1.24G [00:02<02:20, 9.29MB/s]  2%|▏         | 23.0M/1.24G [00:02<02:21, 9.23MB/s]  2%|▏         | 23.9M/1.24G [00:03<02:21, 9.25MB/s]  2%|▏         | 24.8M/1.24G [00:03<02:21, 9.23MB/s]  2%|▏         | 25.7M/1.24G [00:03<02:21, 9.22MB/s]  2%|▏         | 26.6M/1.24G [00:03<02:21, 9.21MB/s]  2%|▏         | 27.5M/1.24G [00:03<02:21, 9.22MB/s]  2%|▏         | 28.4M/1.24G [00:03<02:19, 9.35MB/s]  2%|▏         | 29.3M/1.24G [00:03<02:21, 9.19MB/s]  2%|▏         | 30.2M/1.24G [00:03<02:19, 9.31MB/s]  2%|▏         | 31.1M/1.24G [00:03<02:21, 9.20MB/s]  3%|▎         | 32.0M/1.24G [00:03<02:20, 9.24MB/s]  3%|▎         | 32.9M/1.24G [00:04<02:20, 9.24MB/s]  3%|▎         | 33.8M/1.24G [00:04<02:20, 9.25MB/s]  3%|▎         | 34.7M/1.24G [00:04<02:20, 9.22MB/s]  3%|▎         | 35.6M/1.24G [00:04<02:19, 9.28MB/s]  3%|▎         | 36.5M/1.24G [00:04<02:20, 9.23MB/s]  3%|▎         | 37.3M/1.24G [00:04<02:20, 9.23MB/s]  3%|▎         | 38.2M/1.24G [00:04<02:19, 9.30MB/s]  3%|▎         | 39.1M/1.24G [00:04<02:20, 9.21MB/s]  3%|▎         | 40.1M/1.24G [00:04<02:18, 9.31MB/s]  3%|▎         | 40.9M/1.24G [00:04<02:19, 9.23MB/s]  3%|▎         | 41.8M/1.24G [00:05<02:19, 9.26MB/s]  3%|▎         | 42.7M/1.24G [00:05<02:21, 9.13MB/s]  3%|▎         | 43.6M/1.24G [00:05<02:18, 9.27MB/s]  4%|▎         | 44.5M/1.24G [00:05<02:18, 9.26MB/s]  4%|▎         | 45.4M/1.24G [00:05<02:19, 9.22MB/s]  4%|▎         | 46.3M/1.24G [00:05<02:18, 9.26MB/s]  4%|▎         | 47.2M/1.24G [00:05<02:19, 9.22MB/s]  4%|▍         | 48.1M/1.24G [00:05<02:18, 9.25MB/s]  4%|▍         | 49.0M/1.24G [00:05<02:19, 9.19MB/s]  4%|▍         | 50.0M/1.24G [00:06<02:17, 9.33MB/s]  4%|▍         | 50.8M/1.24G [00:06<02:19, 9.20MB/s]  4%|▍         | 51.8M/1.24G [00:06<02:17, 9.33MB/s]  4%|▍         | 52.7M/1.24G [00:06<02:18, 9.21MB/s]  4%|▍         | 53.6M/1.24G [00:06<02:18, 9.25MB/s]  4%|▍         | 54.4M/1.24G [00:06<02:18, 9.21MB/s]  4%|▍         | 55.3M/1.24G [00:06<02:17, 9.27MB/s]  4%|▍         | 56.2M/1.24G [00:06<02:18, 9.21MB/s]  4%|▍         | 57.1M/1.24G [00:06<02:17, 9.27MB/s]  5%|▍         | 58.0M/1.24G [00:06<02:17, 9.25MB/s]  5%|▍         | 58.9M/1.24G [00:07<02:17, 9.26MB/s]  5%|▍         | 59.8M/1.24G [00:07<02:16, 9.29MB/s]  5%|▍         | 60.7M/1.24G [00:07<02:17, 9.23MB/s]  5%|▍         | 61.6M/1.24G [00:07<02:15, 9.34MB/s]  5%|▍         | 62.5M/1.24G [00:07<02:17, 9.23MB/s]  5%|▍         | 63.4M/1.24G [00:07<02:17, 9.24MB/s]  5%|▌         | 64.3M/1.24G [00:07<02:18, 9.16MB/s]  5%|▌         | 65.2M/1.24G [00:07<02:16, 9.27MB/s]  5%|▌         | 66.1M/1.24G [00:07<02:17, 9.21MB/s]  5%|▌         | 67.0M/1.24G [00:07<02:16, 9.26MB/s]  5%|▌         | 67.9M/1.24G [00:08<02:16, 9.28MB/s]  5%|▌         | 68.8M/1.24G [00:08<02:16, 9.23MB/s]  5%|▌         | 69.7M/1.24G [00:08<02:16, 9.21MB/s]  6%|▌         | 70.6M/1.24G [00:08<02:16, 9.20MB/s]  6%|▌         | 71.5M/1.24G [00:08<02:14, 9.36MB/s]  6%|▌         | 72.4M/1.24G [00:08<02:16, 9.18MB/s]  6%|▌         | 73.3M/1.24G [00:08<02:14, 9.31MB/s]  6%|▌         | 74.2M/1.24G [00:08<02:16, 9.21MB/s]  6%|▌         | 75.1M/1.24G [00:08<02:15, 9.25MB/s]  6%|▌         | 76.0M/1.24G [00:08<02:16, 9.19MB/s]  6%|▌         | 76.9M/1.24G [00:09<02:14, 9.28MB/s]  6%|▌         | 77.8M/1.24G [00:09<02:16, 9.19MB/s]  6%|▌         | 78.7M/1.24G [00:09<02:15, 9.24MB/s]  6%|▋         | 79.6M/1.24G [00:09<02:14, 9.26MB/s]  6%|▋         | 80.5M/1.24G [00:09<02:14, 9.27MB/s]  6%|▋         | 81.4M/1.24G [00:09<02:14, 9.27MB/s]  6%|▋         | 82.2M/1.24G [00:09<02:15, 9.21MB/s]  7%|▋         | 83.2M/1.24G [00:09<02:13, 9.31MB/s]  7%|▋         | 84.1M/1.24G [00:09<02:14, 9.24MB/s]  7%|▋         | 84.9M/1.24G [00:09<02:14, 9.24MB/s]  7%|▋         | 85.8M/1.24G [00:10<02:15, 9.20MB/s]  7%|▋         | 86.7M/1.24G [00:10<02:14, 9.26MB/s]  7%|▋         | 87.6M/1.24G [00:10<02:16, 9.08MB/s]  7%|▋         | 88.5M/1.24G [00:10<02:13, 9.29MB/s]  7%|▋         | 89.4M/1.24G [00:10<02:14, 9.22MB/s]  7%|▋         | 90.3M/1.24G [00:10<02:13, 9.27MB/s]  7%|▋         | 91.2M/1.24G [00:10<02:13, 9.27MB/s]  7%|▋         | 92.1M/1.24G [00:10<02:13, 9.24MB/s]  7%|▋         | 93.0M/1.24G [00:10<02:12, 9.31MB/s]  7%|▋         | 93.9M/1.24G [00:11<02:14, 9.18MB/s]  7%|▋         | 94.8M/1.24G [00:11<02:12, 9.31MB/s]  8%|▊         | 95.7M/1.24G [00:11<02:13, 9.22MB/s]  8%|▊         | 96.6M/1.24G [00:11<02:12, 9.27MB/s]  8%|▊         | 97.5M/1.24G [00:11<02:13, 9.20MB/s]  8%|▊         | 98.4M/1.24G [00:11<02:12, 9.31MB/s]  8%|▊         | 99.3M/1.24G [00:11<02:13, 9.23MB/s]  8%|▊         | 100M/1.24G [00:11<02:12, 9.24MB/s]   8%|▊         | 101M/1.24G [00:11<02:12, 9.23MB/s]  8%|▊         | 102M/1.24G [00:11<02:12, 9.25MB/s]  8%|▊         | 103M/1.24G [00:12<02:11, 9.30MB/s]  8%|▊         | 104M/1.24G [00:12<02:13, 9.20MB/s]  8%|▊         | 105M/1.24G [00:12<02:11, 9.32MB/s]  8%|▊         | 106M/1.24G [00:12<02:12, 9.21MB/s]  8%|▊         | 106M/1.24G [00:12<02:12, 9.21MB/s]  8%|▊         | 107M/1.24G [00:12<02:12, 9.23MB/s]  9%|▊         | 108M/1.24G [00:12<02:11, 9.27MB/s]  9%|▊         | 109M/1.24G [00:12<02:12, 9.21MB/s]  9%|▊         | 110M/1.24G [00:12<02:11, 9.26MB/s]  9%|▊         | 111M/1.24G [00:12<02:12, 9.21MB/s]  9%|▉         | 112M/1.24G [00:13<02:11, 9.23MB/s]  9%|▉         | 113M/1.24G [00:13<02:11, 9.26MB/s]  9%|▉         | 114M/1.24G [00:13<02:11, 9.23MB/s]  9%|▉         | 115M/1.24G [00:13<02:09, 9.35MB/s]  9%|▉         | 115M/1.24G [00:13<02:13, 9.07MB/s]  9%|▉         | 116M/1.24G [00:13<02:10, 9.31MB/s]  9%|▉         | 117M/1.24G [00:13<02:10, 9.26MB/s]  9%|▉         | 118M/1.24G [00:13<02:10, 9.25MB/s]  9%|▉         | 119M/1.24G [00:13<02:10, 9.23MB/s]  9%|▉         | 120M/1.24G [00:13<02:09, 9.29MB/s] 10%|▉         | 121M/1.24G [00:14<02:10, 9.22MB/s] 10%|▉         | 122M/1.24G [00:14<02:10, 9.26MB/s] 10%|▉         | 123M/1.24G [00:14<02:10, 9.22MB/s] 10%|▉         | 124M/1.24G [00:14<02:09, 9.27MB/s] 10%|▉         | 124M/1.24G [00:14<02:09, 9.25MB/s] 10%|▉         | 125M/1.24G [00:14<02:10, 9.21MB/s] 10%|▉         | 126M/1.24G [00:14<02:09, 9.30MB/s] 10%|▉         | 127M/1.24G [00:14<02:10, 9.20MB/s] 10%|█         | 128M/1.24G [00:14<02:09, 9.23MB/s] 10%|█         | 129M/1.24G [00:14<02:09, 9.25MB/s] 10%|█         | 130M/1.24G [00:15<02:08, 9.30MB/s] 10%|█         | 131M/1.24G [00:15<02:09, 9.22MB/s] 10%|█         | 132M/1.24G [00:15<02:08, 9.27MB/s] 10%|█         | 132M/1.24G [00:15<02:09, 9.21MB/s] 10%|█         | 133M/1.24G [00:15<02:09, 9.21MB/s] 11%|█         | 134M/1.24G [00:15<02:08, 9.25MB/s] 11%|█         | 135M/1.24G [00:15<02:08, 9.25MB/s] 11%|█         | 136M/1.24G [00:15<02:07, 9.32MB/s] 11%|█         | 137M/1.24G [00:15<02:09, 9.18MB/s] 11%|█         | 138M/1.24G [00:15<02:07, 9.29MB/s] 11%|█         | 139M/1.24G [00:16<02:08, 9.24MB/s] 11%|█         | 140M/1.24G [00:16<02:08, 9.27MB/s] 11%|█         | 141M/1.24G [00:16<02:16, 8.71MB/s] 11%|█         | 142M/1.24G [00:16<02:05, 9.43MB/s] 11%|█         | 143M/1.24G [00:16<02:13, 8.85MB/s] 11%|█▏        | 144M/1.24G [00:16<02:04, 9.46MB/s] 11%|█▏        | 145M/1.24G [00:16<02:12, 8.91MB/s] 11%|█▏        | 146M/1.24G [00:16<02:04, 9.48MB/s] 12%|█▏        | 147M/1.24G [00:16<02:11, 8.96MB/s] 12%|█▏        | 148M/1.24G [00:17<02:03, 9.50MB/s] 12%|█▏        | 148M/1.24G [00:17<02:11, 8.97MB/s] 12%|█▏        | 150M/1.24G [00:17<02:03, 9.52MB/s] 12%|█▏        | 150M/1.24G [00:17<02:10, 8.98MB/s] 12%|█▏        | 152M/1.24G [00:17<02:03, 9.52MB/s] 12%|█▏        | 152M/1.24G [00:17<02:10, 9.00MB/s] 12%|█▏        | 153M/1.24G [00:17<02:02, 9.54MB/s] 12%|█▏        | 154M/1.24G [00:17<02:10, 8.99MB/s] 12%|█▏        | 155M/1.24G [00:17<02:03, 9.51MB/s] 12%|█▏        | 156M/1.24G [00:18<02:09, 8.99MB/s] 12%|█▏        | 157M/1.24G [00:18<02:02, 9.51MB/s] 12%|█▏        | 158M/1.24G [00:18<02:09, 8.98MB/s] 13%|█▎        | 159M/1.24G [00:18<02:05, 9.28MB/s] 13%|█▎        | 160M/1.24G [00:18<02:09, 8.99MB/s] 13%|█▎        | 161M/1.24G [00:18<02:34, 7.54MB/s] 13%|█▎        | 162M/1.24G [00:18<02:43, 7.11MB/s] 13%|█▎        | 164M/1.24G [00:18<01:55, 10.0MB/s] 13%|█▎        | 165M/1.24G [00:19<01:50, 10.5MB/s] 13%|█▎        | 166M/1.24G [00:19<02:01, 9.57MB/s] 13%|█▎        | 167M/1.24G [00:19<01:54, 10.1MB/s] 13%|█▎        | 168M/1.24G [00:19<02:04, 9.31MB/s] 13%|█▎        | 169M/1.24G [00:19<02:25, 7.93MB/s] 13%|█▎        | 170M/1.24G [00:19<02:10, 8.85MB/s] 13%|█▎        | 171M/1.24G [00:19<01:59, 9.69MB/s] 14%|█▎        | 172M/1.24G [00:19<01:55, 9.97MB/s] 14%|█▎        | 173M/1.24G [00:20<02:03, 9.29MB/s] 14%|█▎        | 174M/1.24G [00:20<02:14, 8.54MB/s] 14%|█▍        | 175M/1.24G [00:20<02:00, 9.51MB/s] 14%|█▍        | 177M/1.24G [00:20<01:54, 10.1MB/s] 14%|█▍        | 178M/1.24G [00:20<02:31, 7.58MB/s] 14%|█▍        | 179M/1.24G [00:20<01:57, 9.74MB/s] 14%|█▍        | 180M/1.24G [00:21<04:00, 4.76MB/s] 14%|█▍        | 181M/1.24G [00:21<03:30, 5.43MB/s] 14%|█▍        | 182M/1.24G [00:21<03:10, 6.01MB/s] 14%|█▍        | 183M/1.24G [00:21<02:50, 6.69MB/s] 14%|█▍        | 184M/1.24G [00:21<02:39, 7.17MB/s] 15%|█▍        | 185M/1.24G [00:21<02:27, 7.74MB/s] 15%|█▍        | 186M/1.24G [00:21<02:21, 8.07MB/s] 15%|█▍        | 186M/1.24G [00:21<02:15, 8.42MB/s] 15%|█▍        | 187M/1.24G [00:22<02:43, 6.95MB/s] 15%|█▍        | 189M/1.24G [00:22<02:00, 9.40MB/s] 15%|█▍        | 190M/1.24G [00:22<02:14, 8.46MB/s] 15%|█▌        | 191M/1.24G [00:22<02:30, 7.54MB/s] 15%|█▌        | 193M/1.24G [00:22<02:18, 8.18MB/s] 15%|█▌        | 194M/1.24G [00:22<01:48, 10.5MB/s] 15%|█▌        | 195M/1.24G [00:23<02:06, 8.95MB/s] 15%|█▌        | 197M/1.24G [00:23<01:50, 10.2MB/s] 16%|█▌        | 198M/1.24G [00:23<01:52, 9.99MB/s] 16%|█▌        | 199M/1.24G [00:23<01:55, 9.74MB/s] 16%|█▌        | 200M/1.24G [00:23<01:56, 9.68MB/s] 16%|█▌        | 201M/1.24G [00:23<01:58, 9.47MB/s] 16%|█▌        | 202M/1.24G [00:23<01:58, 9.46MB/s] 16%|█▌        | 203M/1.24G [00:23<01:59, 9.36MB/s] 16%|█▌        | 204M/1.24G [00:23<01:59, 9.40MB/s] 16%|█▌        | 205M/1.24G [00:24<01:58, 9.42MB/s] 16%|█▌        | 205M/1.24G [00:24<02:00, 9.28MB/s] 16%|█▌        | 206M/1.24G [00:24<01:59, 9.33MB/s] 16%|█▋        | 207M/1.24G [00:24<02:00, 9.24MB/s] 16%|█▋        | 208M/1.24G [00:24<02:00, 9.27MB/s] 16%|█▋        | 209M/1.24G [00:24<02:00, 9.21MB/s] 17%|█▋        | 210M/1.24G [00:24<01:59, 9.31MB/s] 17%|█▋        | 211M/1.24G [00:24<02:00, 9.20MB/s] 17%|█▋        | 212M/1.24G [00:24<02:00, 9.24MB/s] 17%|█▋        | 213M/1.24G [00:24<02:00, 9.20MB/s] 17%|█▋        | 214M/1.24G [00:25<01:59, 9.26MB/s] 17%|█▋        | 214M/1.24G [00:25<02:00, 9.21MB/s] 17%|█▋        | 215M/1.24G [00:25<01:59, 9.26MB/s] 17%|█▋        | 216M/1.24G [00:25<01:58, 9.33MB/s] 17%|█▋        | 217M/1.24G [00:25<01:59, 9.21MB/s] 17%|█▋        | 218M/1.24G [00:25<01:58, 9.32MB/s] 17%|█▋        | 219M/1.24G [00:25<02:00, 9.19MB/s] 17%|█▋        | 220M/1.24G [00:25<01:58, 9.31MB/s] 17%|█▋        | 221M/1.24G [00:25<01:59, 9.18MB/s] 17%|█▋        | 222M/1.24G [00:25<01:58, 9.32MB/s] 18%|█▊        | 223M/1.24G [00:26<01:59, 9.17MB/s] 18%|█▊        | 223M/1.24G [00:26<01:58, 9.26MB/s] 18%|█▊        | 224M/1.24G [00:26<01:59, 9.16MB/s] 18%|█▊        | 225M/1.24G [00:26<01:58, 9.27MB/s] 18%|█▊        | 226M/1.24G [00:26<01:58, 9.24MB/s] 18%|█▊        | 227M/1.24G [00:26<01:58, 9.24MB/s] 18%|█▊        | 228M/1.24G [00:26<01:59, 9.15MB/s] 18%|█▊        | 229M/1.24G [00:26<01:59, 9.12MB/s] 18%|█▊        | 230M/1.24G [00:26<01:56, 9.40MB/s] 18%|█▊        | 231M/1.24G [00:27<02:00, 9.05MB/s] 18%|█▊        | 232M/1.24G [00:27<01:56, 9.37MB/s] 18%|█▊        | 233M/1.24G [00:27<02:00, 9.02MB/s] 18%|█▊        | 234M/1.24G [00:27<01:55, 9.44MB/s] 18%|█▊        | 234M/1.24G [00:27<02:00, 9.03MB/s] 19%|█▊        | 235M/1.24G [00:27<01:55, 9.41MB/s] 19%|█▊        | 236M/1.24G [00:27<01:57, 9.25MB/s] 19%|█▊        | 237M/1.24G [00:27<01:55, 9.35MB/s] 19%|█▊        | 238M/1.24G [00:27<01:57, 9.21MB/s] 19%|█▉        | 239M/1.24G [00:27<01:55, 9.35MB/s] 19%|█▉        | 240M/1.24G [00:28<01:57, 9.18MB/s] 19%|█▉        | 241M/1.24G [00:28<01:56, 9.29MB/s] 19%|█▉        | 242M/1.24G [00:28<01:56, 9.23MB/s] 19%|█▉        | 243M/1.24G [00:28<01:56, 9.23MB/s] 19%|█▉        | 244M/1.24G [00:28<01:55, 9.35MB/s] 19%|█▉        | 245M/1.24G [00:28<01:58, 9.11MB/s] 19%|█▉        | 246M/1.24G [00:28<01:55, 9.34MB/s] 19%|█▉        | 246M/1.24G [00:28<01:58, 9.05MB/s] 19%|█▉        | 247M/1.24G [00:28<01:54, 9.38MB/s] 20%|█▉        | 248M/1.24G [00:28<01:58, 9.02MB/s] 20%|█▉        | 249M/1.24G [00:29<01:54, 9.39MB/s] 20%|█▉        | 250M/1.24G [00:29<01:58, 9.03MB/s] 20%|█▉        | 251M/1.24G [00:29<01:53, 9.40MB/s] 20%|█▉        | 252M/1.24G [00:29<01:54, 9.30MB/s] 20%|█▉        | 253M/1.24G [00:29<01:54, 9.33MB/s] 20%|█▉        | 254M/1.24G [00:29<01:56, 9.19MB/s] 20%|██        | 255M/1.24G [00:29<01:54, 9.34MB/s] 20%|██        | 256M/1.24G [00:29<01:56, 9.18MB/s] 20%|██        | 257M/1.24G [00:29<01:54, 9.29MB/s] 20%|██        | 258M/1.24G [00:30<02:30, 7.05MB/s] 20%|██        | 259M/1.24G [00:30<01:45, 10.1MB/s] 20%|██        | 260M/1.24G [00:30<01:48, 9.74MB/s] 21%|██        | 261M/1.24G [00:30<02:04, 8.52MB/s] 21%|██        | 263M/1.24G [00:30<02:15, 7.82MB/s] 21%|██        | 264M/1.24G [00:30<02:49, 6.22MB/s] 21%|██        | 266M/1.24G [00:31<01:42, 10.3MB/s] 21%|██        | 267M/1.24G [00:31<01:35, 11.0MB/s] 21%|██        | 269M/1.24G [00:31<01:50, 9.53MB/s] 21%|██        | 270M/1.24G [00:31<01:42, 10.3MB/s] 21%|██▏       | 271M/1.24G [00:31<01:47, 9.75MB/s] 21%|██▏       | 272M/1.24G [00:31<01:44, 9.98MB/s] 21%|██▏       | 273M/1.24G [00:31<02:03, 8.48MB/s] 22%|██▏       | 274M/1.24G [00:32<02:13, 7.83MB/s] 22%|██▏       | 276M/1.24G [00:32<01:40, 10.4MB/s] 22%|██▏       | 277M/1.24G [00:32<01:42, 10.1MB/s] 22%|██▏       | 278M/1.24G [00:32<01:44, 9.93MB/s] 22%|██▏       | 279M/1.24G [00:32<01:47, 9.70MB/s] 22%|██▏       | 280M/1.24G [00:32<02:10, 7.96MB/s] 22%|██▏       | 282M/1.24G [00:32<01:45, 9.80MB/s] 22%|██▏       | 283M/1.24G [00:32<01:44, 9.96MB/s] 22%|██▏       | 284M/1.24G [00:33<01:49, 9.49MB/s] 22%|██▏       | 285M/1.24G [00:33<01:47, 9.66MB/s] 22%|██▏       | 286M/1.24G [00:33<01:51, 9.27MB/s] 23%|██▎       | 287M/1.24G [00:33<01:48, 9.54MB/s] 23%|██▎       | 288M/1.24G [00:33<01:52, 9.16MB/s] 23%|██▎       | 289M/1.24G [00:33<01:48, 9.50MB/s] 23%|██▎       | 290M/1.24G [00:33<01:52, 9.11MB/s] 23%|██▎       | 291M/1.24G [00:33<01:48, 9.46MB/s] 23%|██▎       | 291M/1.24G [00:33<01:53, 9.08MB/s] 23%|██▎       | 292M/1.24G [00:33<01:48, 9.44MB/s] 23%|██▎       | 293M/1.24G [00:34<01:53, 9.04MB/s] 23%|██▎       | 294M/1.24G [00:34<01:48, 9.43MB/s] 23%|██▎       | 295M/1.24G [00:34<01:52, 9.07MB/s] 23%|██▎       | 296M/1.24G [00:34<01:48, 9.47MB/s] 23%|██▎       | 297M/1.24G [00:34<01:52, 9.05MB/s] 23%|██▎       | 298M/1.24G [00:34<01:48, 9.44MB/s] 24%|██▎       | 299M/1.24G [00:34<01:52, 9.08MB/s] 24%|██▎       | 300M/1.24G [00:34<01:48, 9.39MB/s] 24%|██▎       | 301M/1.24G [00:34<01:51, 9.13MB/s] 24%|██▎       | 302M/1.24G [00:35<01:48, 9.36MB/s] 24%|██▍       | 303M/1.24G [00:35<01:49, 9.28MB/s] 24%|██▍       | 304M/1.24G [00:35<01:49, 9.29MB/s] 24%|██▍       | 305M/1.24G [00:35<01:49, 9.22MB/s] 24%|██▍       | 305M/1.24G [00:35<01:49, 9.22MB/s] 24%|██▍       | 306M/1.24G [00:35<01:48, 9.30MB/s] 24%|██▍       | 307M/1.24G [00:35<01:51, 9.10MB/s] 24%|██▍       | 308M/1.24G [00:35<01:47, 9.43MB/s] 24%|██▍       | 309M/1.24G [00:35<01:51, 9.05MB/s] 24%|██▍       | 310M/1.24G [00:36<01:46, 9.42MB/s] 24%|██▍       | 311M/1.24G [00:36<01:51, 9.06MB/s] 25%|██▍       | 312M/1.24G [00:36<01:46, 9.42MB/s] 25%|██▍       | 313M/1.24G [00:36<01:51, 9.05MB/s] 25%|██▍       | 314M/1.24G [00:36<01:46, 9.43MB/s] 25%|██▍       | 315M/1.24G [00:36<01:50, 9.04MB/s] 25%|██▍       | 316M/1.24G [00:36<01:46, 9.44MB/s] 25%|██▍       | 317M/1.24G [00:36<01:50, 9.05MB/s] 25%|██▍       | 318M/1.24G [00:36<01:46, 9.38MB/s] 25%|██▌       | 319M/1.24G [00:36<01:49, 9.11MB/s] 25%|██▌       | 320M/1.24G [00:37<01:46, 9.41MB/s] 25%|██▌       | 321M/1.24G [00:37<01:48, 9.17MB/s] 25%|██▌       | 321M/1.24G [00:37<01:46, 9.31MB/s] 25%|██▌       | 322M/1.24G [00:37<01:47, 9.27MB/s] 25%|██▌       | 323M/1.24G [00:37<01:47, 9.28MB/s] 25%|██▌       | 324M/1.24G [00:37<01:47, 9.23MB/s] 26%|██▌       | 325M/1.24G [00:37<01:48, 9.18MB/s] 26%|██▌       | 326M/1.24G [00:37<01:46, 9.32MB/s] 26%|██▌       | 327M/1.24G [00:37<01:48, 9.12MB/s] 26%|██▌       | 328M/1.24G [00:38<01:44, 9.44MB/s] 26%|██▌       | 329M/1.24G [00:38<01:48, 9.09MB/s] 26%|██▌       | 330M/1.24G [00:38<01:44, 9.42MB/s] 26%|██▌       | 331M/1.24G [00:38<01:49, 9.03MB/s] 26%|██▌       | 332M/1.24G [00:38<01:44, 9.42MB/s] 26%|██▌       | 333M/1.24G [00:38<01:48, 9.05MB/s] 26%|██▌       | 334M/1.24G [00:38<01:44, 9.42MB/s] 26%|██▋       | 334M/1.24G [00:38<01:48, 9.04MB/s] 26%|██▋       | 335M/1.24G [00:38<01:44, 9.40MB/s] 26%|██▋       | 336M/1.24G [00:38<01:48, 9.07MB/s] 27%|██▋       | 337M/1.24G [00:39<01:43, 9.44MB/s] 27%|██▋       | 338M/1.24G [00:39<01:47, 9.13MB/s] 27%|██▋       | 339M/1.24G [00:39<01:44, 9.36MB/s] 27%|██▋       | 340M/1.24G [00:39<01:46, 9.18MB/s] 27%|██▋       | 341M/1.24G [00:39<01:44, 9.33MB/s] 27%|██▋       | 342M/1.24G [00:39<01:45, 9.22MB/s] 27%|██▋       | 343M/1.24G [00:39<01:45, 9.24MB/s] 27%|██▋       | 344M/1.24G [00:39<01:45, 9.23MB/s] 27%|██▋       | 345M/1.24G [00:39<01:46, 9.15MB/s] 27%|██▋       | 346M/1.24G [00:40<01:44, 9.33MB/s] 27%|██▋       | 346M/1.24G [00:40<01:46, 9.12MB/s] 27%|██▋       | 347M/1.24G [00:40<01:42, 9.48MB/s] 27%|██▋       | 348M/1.24G [00:40<01:46, 9.09MB/s] 27%|██▋       | 349M/1.24G [00:40<01:42, 9.39MB/s] 28%|██▊       | 350M/1.24G [00:40<01:46, 9.09MB/s] 28%|██▊       | 351M/1.24G [00:40<01:42, 9.39MB/s] 28%|██▊       | 352M/1.24G [00:40<01:46, 9.07MB/s] 28%|██▊       | 353M/1.24G [00:40<01:42, 9.42MB/s] 28%|██▊       | 354M/1.24G [00:40<01:46, 9.03MB/s] 28%|██▊       | 354M/1.24G [00:41<01:46, 9.03MB/s]
Traceback (most recent call last):
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/hub.py", line 618, in download_url_to_file
    f.write(buffer)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/tempfile.py", line 481, in func_wrapper
    return func(*args, **kwargs)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 242, in <module>
    main()
  File "train.py", line 238, in main
    meta=meta)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/apis/train.py", line 193, in train_segmentor
    runner.load_checkpoint(cfg.load_from)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_runner.py", line 355, in load_checkpoint
    revise_keys=revise_keys)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 627, in load_checkpoint
    checkpoint = _load_checkpoint(filename, map_location, logger)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 561, in _load_checkpoint
    return CheckpointLoader.load_checkpoint(filename, map_location, logger)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 303, in load_checkpoint
    return checkpoint_loader(filename, map_location)  # type: ignore
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 348, in load_from_http
    filename, model_dir=model_dir, map_location=map_location)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/hub.py", line 727, in load_state_dict_from_url
    download_url_to_file(url, cached_file, hash_prefix, progress=progress)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/hub.py", line 631, in download_url_to_file
    f.close()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/tempfile.py", line 507, in close
    self._closer.close()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/tempfile.py", line 441, in close
    self.file.close()
OSError: [Errno 28] No space left on device
2022-09-21 17:26:39,774 - mmseg - INFO - Multi-processing start method is `None`
2022-09-21 17:26:39,777 - mmseg - INFO - OpenCV num_threads is `48
2022-09-21 17:26:39,984 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.27.0+45dfd3f
------------------------------------------------------------

2022-09-21 17:26:39,985 - mmseg - INFO - Distributed training: False
2022-09-21 17:26:41,714 - mmseg - INFO - Config:
checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth'
backbone_norm_cfg = dict(type='LN', eps=1e-06, requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained=
    'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=(640, 640),
        patch_size=16,
        in_channels=3,
        embed_dims=1024,
        num_layers=24,
        num_heads=16,
        drop_path_rate=0.1,
        attn_drop_rate=0.0,
        drop_rate=0.0,
        final_norm=True,
        norm_cfg=dict(type='LN', eps=1e-06, requires_grad=True),
        with_cls_token=True,
        interpolate_mode='bicubic'),
    decode_head=dict(
        type='SegmenterMaskTransformerHead',
        in_channels=1024,
        channels=1024,
        num_classes=2,
        num_layers=2,
        num_heads=16,
        embed_dims=1024,
        dropout_ratio=0.0,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    test_cfg=dict(mode='slide', crop_size=(640, 640), stride=(608, 608)))
dataset_type = 'NematodosDataset'
data_root = '../data/nematodos'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (768, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
    dict(type='RandomRotate', prob=0.75, degree=30),
    dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 768),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=2,
    train=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomRotate', prob=0.75, degree=30),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomRotate', prob=0.75, degree=30),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/test.txt'),
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=2, shuffle=False))
log_config = dict(
    interval=16000,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False, interval=8000),
        dict(
            type='MMSegWandbHook',
            with_step=False,
            init_kwargs=dict(
                entity='seg_nematodos',
                project='Nematodos',
                name='segmenterL_A2',
                id='segmenterL_A2',
                resume='allow',
                notes=
                'Entrenamiento modelo segmenterL pretrain, aumentado 2, batch=1, lr=0.001, m=0.9, 160k iter'
            ),
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'https://download.openmmlab.com/mmsegmentation/v0.5/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth'
resume_from = None
workflow = [('train', 1), ('val', 1)]
cudnn_benchmark = True
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0)
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000, max_keep_ckpts=2)
evaluation = dict(
    interval=16000, metric=['mIoU', 'mDice', 'mFscore'], pre_eval=True)
work_dir = '../work_dirs/segmenterL_A2'
seed = 0
gpu_ids = [0]
device = 'cuda'
auto_resume = False

2022-09-21 17:26:41,715 - mmseg - INFO - Set random seed to 1683619953, deterministic: False
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
2022-09-21 17:26:43,923 - mmseg - INFO - load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth
2022-09-21 17:26:44,439 - mmseg - INFO - Resize the pos_embed shape from torch.Size([1, 577, 1024]) to torch.Size([1, 1601, 1024])
train.py:207: UserWarning: SyncBN is only supported with DDP. To be compatible with DP, we convert SyncBN to BN. Please use dist_train.sh which can avoid this error.
  'SyncBN is only supported with DDP. To be compatible with DP, '
2022-09-21 17:26:45,720 - mmseg - INFO - EncoderDecoder(
  (backbone): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (12): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (13): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (14): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (15): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (16): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (17): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (18): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (19): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (20): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (21): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (22): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (23): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segmenter/vit_large_p16_384_20220308-d4efb41d.pth'}
  (decode_head): SegmenterMaskTransformerHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate=none)
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=4096, out_features=1024, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (dec_proj): Linear(in_features=1024, out_features=1024, bias=True)
    (patch_proj): Linear(in_features=1024, out_features=1024, bias=False)
    (classes_proj): Linear(in_features=1024, out_features=1024, bias=False)
    (decoder_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (mask_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2022-09-21 17:26:45,729 - mmseg - INFO - Loaded 2296 images
2022-09-21 17:26:45,731 - mmseg - INFO - Loaded 764 images
2022-09-21 17:26:49,553 - mmseg - INFO - Loaded 764 images
2022-09-21 17:26:49,554 - mmseg - INFO - load checkpoint from http path: https://download.openmmlab.com/mmsegmentation/v0.5/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth
Downloading: "https://download.openmmlab.com/mmsegmentation/v0.5/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth" to /home/kcarvajal/.cache/torch/hub/checkpoints/segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth
  0%|          | 0.00/1.24G [00:00<?, ?B/s]  0%|          | 56.0k/1.24G [00:00<41:09, 540kB/s]  0%|          | 112k/1.24G [00:00<41:29, 535kB/s]   0%|          | 232k/1.24G [00:00<26:52, 827kB/s]  0%|          | 456k/1.24G [00:00<16:04, 1.38MB/s]  0%|          | 896k/1.24G [00:00<08:57, 2.48MB/s]  0%|          | 1.73M/1.24G [00:00<04:48, 4.62MB/s]  0%|          | 3.20M/1.24G [00:00<02:43, 8.12MB/s]  0%|          | 3.99M/1.24G [00:00<02:43, 8.12MB/s]  0%|          | 4.91M/1.24G [00:00<02:34, 8.59MB/s]  0%|          | 5.74M/1.24G [00:01<02:39, 8.34MB/s]  1%|          | 6.70M/1.24G [00:01<02:31, 8.77MB/s]  1%|          | 7.54M/1.24G [00:01<02:37, 8.42MB/s]  1%|          | 8.43M/1.24G [00:01<02:32, 8.67MB/s]  1%|          | 9.27M/1.24G [00:01<02:35, 8.52MB/s]  1%|          | 10.1M/1.24G [00:01<02:35, 8.51MB/s]  1%|          | 11.0M/1.24G [00:01<02:31, 8.72MB/s]  1%|          | 11.8M/1.24G [00:01<02:34, 8.52MB/s]  1%|          | 12.7M/1.24G [00:01<02:28, 8.88MB/s]  1%|          | 13.6M/1.24G [00:01<02:34, 8.54MB/s]  1%|          | 14.5M/1.24G [00:02<02:29, 8.84MB/s]  1%|          | 15.4M/1.24G [00:02<02:34, 8.55MB/s]  1%|▏         | 16.2M/1.24G [00:02<02:32, 8.61MB/s]  1%|▏         | 17.0M/1.24G [00:02<02:31, 8.68MB/s]  1%|▏         | 17.9M/1.24G [00:02<02:33, 8.57MB/s]  1%|▏         | 18.8M/1.24G [00:02<02:29, 8.80MB/s]  2%|▏         | 19.6M/1.24G [00:02<02:33, 8.57MB/s]  2%|▏         | 20.5M/1.24G [00:02<02:28, 8.85MB/s]  2%|▏         | 21.4M/1.24G [00:02<02:33, 8.54MB/s]  2%|▏         | 22.3M/1.24G [00:03<02:29, 8.76MB/s]  2%|▏         | 23.1M/1.24G [00:03<02:33, 8.53MB/s]  2%|▏         | 24.0M/1.24G [00:03<02:32, 8.55MB/s]  2%|▏         | 24.8M/1.24G [00:03<02:30, 8.70MB/s]  2%|▏         | 25.7M/1.24G [00:03<02:31, 8.60MB/s]  2%|▏         | 26.6M/1.24G [00:03<02:27, 8.83MB/s]  2%|▏         | 27.4M/1.24G [00:03<02:32, 8.57MB/s]  2%|▏         | 28.3M/1.24G [00:03<02:26, 8.87MB/s]  2%|▏         | 29.2M/1.24G [00:03<02:32, 8.54MB/s]  2%|▏         | 30.1M/1.24G [00:03<02:29, 8.70MB/s]  2%|▏         | 30.9M/1.24G [00:04<02:31, 8.60MB/s]  2%|▏         | 31.7M/1.24G [00:04<02:32, 8.52MB/s]  3%|▎         | 32.6M/1.24G [00:04<02:28, 8.75MB/s]  3%|▎         | 33.5M/1.24G [00:04<02:30, 8.61MB/s]  3%|▎         | 34.4M/1.24G [00:04<02:26, 8.84MB/s]  3%|▎         | 35.2M/1.24G [00:04<02:31, 8.57MB/s]  3%|▎         | 36.1M/1.24G [00:04<02:26, 8.83MB/s]  3%|▎         | 37.0M/1.24G [00:04<02:31, 8.57MB/s]  3%|▎         | 37.8M/1.24G [00:04<02:30, 8.60MB/s]  3%|▎         | 38.6M/1.24G [00:05<02:29, 8.66MB/s]  3%|▎         | 39.5M/1.24G [00:05<02:31, 8.52MB/s]  3%|▎         | 40.4M/1.24G [00:05<02:26, 8.81MB/s]  3%|▎         | 41.2M/1.24G [00:05<02:30, 8.59MB/s]  3%|▎         | 42.1M/1.24G [00:05<02:25, 8.86MB/s]  3%|▎         | 43.0M/1.24G [00:05<02:30, 8.58MB/s]  3%|▎         | 43.9M/1.24G [00:05<02:26, 8.76MB/s]  3%|▎         | 44.0M/1.24G [00:05<02:37, 8.15MB/s]
Traceback (most recent call last):
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/hub.py", line 618, in download_url_to_file
    f.write(buffer)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/tempfile.py", line 481, in func_wrapper
    return func(*args, **kwargs)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 242, in <module>
    main()
  File "train.py", line 238, in main
    meta=meta)
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/apis/train.py", line 193, in train_segmentor
    runner.load_checkpoint(cfg.load_from)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_runner.py", line 355, in load_checkpoint
    revise_keys=revise_keys)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 627, in load_checkpoint
    checkpoint = _load_checkpoint(filename, map_location, logger)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 561, in _load_checkpoint
    return CheckpointLoader.load_checkpoint(filename, map_location, logger)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 303, in load_checkpoint
    return checkpoint_loader(filename, map_location)  # type: ignore
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 348, in load_from_http
    filename, model_dir=model_dir, map_location=map_location)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/hub.py", line 727, in load_state_dict_from_url
    download_url_to_file(url, cached_file, hash_prefix, progress=progress)
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/hub.py", line 631, in download_url_to_file
    f.close()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/tempfile.py", line 507, in close
    self._closer.close()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/tempfile.py", line 441, in close
    self.file.close()
OSError: [Errno 28] No space left on device
2022-09-21 17:27:02,422 - mmseg - INFO - Multi-processing start method is `None`
2022-09-21 17:27:02,424 - mmseg - INFO - OpenCV num_threads is `48
2022-09-21 17:27:02,614 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.27.0+45dfd3f
------------------------------------------------------------

2022-09-21 17:27:02,614 - mmseg - INFO - Distributed training: False
2022-09-21 17:27:03,036 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/vit-b16_p16_224-80ecf9dd.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=224,
        embed_dims=768,
        num_layers=12,
        num_heads=12,
        out_indices=(2, 5, 8, 11),
        final_norm=False,
        with_cls_token=True,
        output_cls_token=True),
    decode_head=dict(
        type='DPTHead',
        in_channels=(768, 768, 768, 768),
        channels=256,
        embed_dims=768,
        post_process_channels=[96, 192, 384, 768],
        num_classes=2,
        readout_type='project',
        input_transform='multiple_select',
        in_index=(0, 1, 2, 3),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    auxiliary_head=None,
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'NematodosDataset'
data_root = '../data/nematodos'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (768, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
    dict(type='RandomRotate', prob=0.75, degree=30),
    dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 768),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/test.txt'),
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=1, shuffle=False))
log_config = dict(
    interval=16000,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False, interval=8000),
        dict(
            type='MMSegWandbHook',
            with_step=False,
            init_kwargs=dict(
                entity='seg_nematodos',
                project='Nematodos',
                name='dpt_base',
                id='dpt_base',
                resume='allow',
                notes=
                'Entrenamiento modelo dpt base, batch=2, lr=6e-05, 160k iter'),
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1), ('val', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_embed=dict(decay_mult=0.0),
            cls_token=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000, max_keep_ckpts=2)
evaluation = dict(
    interval=16000, metric=['mIoU', 'mDice', 'mFscore'], pre_eval=True)
work_dir = '../work_dirs/dpt'
seed = 0
gpu_ids = [0]
device = 'cuda'
auto_resume = False

2022-09-21 17:27:03,036 - mmseg - INFO - Set random seed to 540015942, deterministic: False
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
2022-09-21 17:27:03,970 - mmseg - INFO - load checkpoint from local path: pretrain/vit-b16_p16_224-80ecf9dd.pth
Traceback (most recent call last):
  File "train.py", line 242, in <module>
    main()
  File "train.py", line 202, in main
    model.init_weights()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_module.py", line 117, in init_weights
    m.init_weights()
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 298, in init_weights
    self.init_cfg['checkpoint'], logger=logger, map_location='cpu')
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 303, in load_checkpoint
    return checkpoint_loader(filename, map_location)  # type: ignore
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 322, in load_from_local
    raise FileNotFoundError(f'{filename} can not be found.')
FileNotFoundError: pretrain/vit-b16_p16_224-80ecf9dd.pth can not be found.
2022-09-21 17:27:09,114 - mmseg - INFO - Multi-processing start method is `None`
2022-09-21 17:27:09,116 - mmseg - INFO - OpenCV num_threads is `48
2022-09-21 17:27:09,368 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.27.0+45dfd3f
------------------------------------------------------------

2022-09-21 17:27:09,368 - mmseg - INFO - Distributed training: False
2022-09-21 17:27:10,024 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/vit-b16_p16_224-80ecf9dd.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=224,
        embed_dims=768,
        num_layers=12,
        num_heads=12,
        out_indices=(2, 5, 8, 11),
        final_norm=False,
        with_cls_token=True,
        output_cls_token=True),
    decode_head=dict(
        type='DPTHead',
        in_channels=(768, 768, 768, 768),
        channels=256,
        embed_dims=768,
        post_process_channels=[96, 192, 384, 768],
        num_classes=2,
        readout_type='project',
        input_transform='multiple_select',
        in_index=(0, 1, 2, 3),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    auxiliary_head=None,
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'NematodosDataset'
data_root = '../data/nematodos'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (768, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
    dict(type='RandomRotate', prob=0.75, degree=30),
    dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 768),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/test.txt'),
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=1, shuffle=False))
log_config = dict(
    interval=16000,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False, interval=8000),
        dict(
            type='MMSegWandbHook',
            with_step=False,
            init_kwargs=dict(
                entity='seg_nematodos',
                project='Nematodos',
                name='dpt_pretrain',
                id='dpt_pretrain',
                resume='allow',
                notes=
                'Entrenamiento modelo dpt preentrenado, batch=2, lr=6e-05, 160k iter'
            ),
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'https://download.openmmlab.com/mmsegmentation/v0.5/dpt/dpt_vit-b16_512x512_160k_ade20k/dpt_vit-b16_512x512_160k_ade20k-db31cf52.pth'
resume_from = None
workflow = [('train', 1), ('val', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_embed=dict(decay_mult=0.0),
            cls_token=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000, max_keep_ckpts=2)
evaluation = dict(
    interval=16000, metric=['mIoU', 'mDice', 'mFscore'], pre_eval=True)
work_dir = '../work_dirs/dpt_pretrain'
seed = 0
gpu_ids = [0]
device = 'cuda'
auto_resume = False

2022-09-21 17:27:10,024 - mmseg - INFO - Set random seed to 11719222, deterministic: False
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
2022-09-21 17:27:10,976 - mmseg - INFO - load checkpoint from local path: pretrain/vit-b16_p16_224-80ecf9dd.pth
Traceback (most recent call last):
  File "train.py", line 242, in <module>
    main()
  File "train.py", line 202, in main
    model.init_weights()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_module.py", line 117, in init_weights
    m.init_weights()
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 298, in init_weights
    self.init_cfg['checkpoint'], logger=logger, map_location='cpu')
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 303, in load_checkpoint
    return checkpoint_loader(filename, map_location)  # type: ignore
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 322, in load_from_local
    raise FileNotFoundError(f'{filename} can not be found.')
FileNotFoundError: pretrain/vit-b16_p16_224-80ecf9dd.pth can not be found.
2022-09-21 17:27:17,837 - mmseg - INFO - Multi-processing start method is `None`
2022-09-21 17:27:17,839 - mmseg - INFO - OpenCV num_threads is `48
2022-09-21 17:27:18,049 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMSegmentation: 0.27.0+45dfd3f
------------------------------------------------------------

2022-09-21 17:27:18,050 - mmseg - INFO - Distributed training: False
2022-09-21 17:27:19,837 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='pretrain/vit-b16_p16_224-80ecf9dd.pth',
    backbone=dict(
        type='VisionTransformer',
        img_size=224,
        embed_dims=768,
        num_layers=12,
        num_heads=12,
        out_indices=(2, 5, 8, 11),
        final_norm=False,
        with_cls_token=True,
        output_cls_token=True),
    decode_head=dict(
        type='DPTHead',
        in_channels=(768, 768, 768, 768),
        channels=256,
        embed_dims=768,
        post_process_channels=[96, 192, 384, 768],
        num_classes=2,
        readout_type='project',
        input_transform='multiple_select',
        in_index=(0, 1, 2, 3),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    auxiliary_head=None,
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'NematodosDataset'
data_root = '../data/nematodos'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (768, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
    dict(type='RandomRotate', prob=0.75, degree=30),
    dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 768),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(type='Resize', img_scale=(1024, 768), ratio_range=(0.5, 1.5)),
            dict(type='RandomRotate', prob=0.75, degree=30),
            dict(type='RandomCrop', crop_size=(768, 1024), cat_max_ratio=0.25),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(768, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomRotate', prob=0.75, degree=30),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='NematodosDataset',
        data_root='../data/nematodos',
        img_dir='images',
        ann_dir='annotations',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 768),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomRotate', prob=0.75, degree=30),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/test.txt'),
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=1, shuffle=False))
log_config = dict(
    interval=16000,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False, interval=8000),
        dict(
            type='MMSegWandbHook',
            with_step=False,
            init_kwargs=dict(
                entity='seg_nematodos',
                project='Nematodos',
                name='dpt_A2',
                id='dpt_A2',
                resume='allow',
                notes=
                'Entrenamiento modelo dpt preentrenado, aumentado 2, batch=2, lr=6e-05, 160k iter'
            ),
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'https://download.openmmlab.com/mmsegmentation/v0.5/dpt/dpt_vit-b16_512x512_160k_ade20k/dpt_vit-b16_512x512_160k_ade20k-db31cf52.pth'
resume_from = None
workflow = [('train', 1), ('val', 1)]
cudnn_benchmark = True
optimizer = dict(
    type='AdamW',
    lr=6e-05,
    betas=(0.9, 0.999),
    weight_decay=0.01,
    paramwise_cfg=dict(
        custom_keys=dict(
            pos_embed=dict(decay_mult=0.0),
            cls_token=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict()
lr_config = dict(
    policy='poly',
    warmup='linear',
    warmup_iters=1500,
    warmup_ratio=1e-06,
    power=1.0,
    min_lr=0.0,
    by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000, max_keep_ckpts=2)
evaluation = dict(
    interval=16000, metric=['mIoU', 'mDice', 'mFscore'], pre_eval=True)
work_dir = '../work_dirs/dpt_A2'
seed = 0
gpu_ids = [0]
device = 'cuda'
auto_resume = False

2022-09-21 17:27:19,837 - mmseg - INFO - Set random seed to 1751173743, deterministic: False
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py:219: UserWarning: DeprecationWarning: pretrained is deprecated, please use "init_cfg" instead
  warnings.warn('DeprecationWarning: pretrained is deprecated, '
/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  'Default ``avg_non_ignore`` is False, if you would like to '
2022-09-21 17:27:20,706 - mmseg - INFO - load checkpoint from local path: pretrain/vit-b16_p16_224-80ecf9dd.pth
Traceback (most recent call last):
  File "train.py", line 242, in <module>
    main()
  File "train.py", line 202, in main
    model.init_weights()
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_module.py", line 117, in init_weights
    m.init_weights()
  File "/work/kcarvajal/mmsegmentation_Nematodos/mmseg/models/backbones/vit.py", line 298, in init_weights
    self.init_cfg['checkpoint'], logger=logger, map_location='cpu')
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 303, in load_checkpoint
    return checkpoint_loader(filename, map_location)  # type: ignore
  File "/home/kcarvajal/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/checkpoint.py", line 322, in load_from_local
    raise FileNotFoundError(f'{filename} can not be found.')
FileNotFoundError: pretrain/vit-b16_p16_224-80ecf9dd.pth can not be found.
